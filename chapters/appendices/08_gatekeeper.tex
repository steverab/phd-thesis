\chapter{Gatekeeper: Improved Model Cascades Through Confidence Tuning}

\section{Broader Impact}
\label{sec:broader_impact}

This work contributes to the responsible and efficient deployment of machine learning systems by improving the decision-making capabilities of smaller, local models in model cascade architectures. By introducing a loss function that calibrates model confidence with respect to correctness, our approach enhances both the performance and transparency of automated systems that must decide when to act autonomously and when to defer to a more capable model. This design can improve the accessibility and sustainability of machine learning applications by reducing reliance on large, energy-intensive models—particularly important in low-resource environments or edge computing.

At the same time, the ability to fine-tune smaller models to strategically abstain from uncertain predictions raises important considerations for fairness and accountability. In high-stakes applications such as healthcare or finance, improper tuning of the deferral threshold—or uncalibrated confidence estimates—could lead to the systematic denial of service or misallocation of computational resources. Care must be taken to ensure that such systems are thoroughly evaluated not only for average performance but also for differential performance across subgroups. Moreover, the use of large models as fallback decision-makers assumes their correctness, which may not always hold, especially in underrepresented domains. We therefore encourage developers and practitioners to accompany deployments of cascade-based systems with rigorous audits of fairness, reliability, and alignment with human values.

\section{Additional Background}

\subsection{Related Work}
\label{sec:ext_rel_work}

\subsubsection{LLM Routing}
\citet{ding2024hybrid} propose a hybrid LLM inference pipeline that routes each query either to a small on-device model or a larger high-quality model based on the query’s predicted difficulty and a tunable quality threshold. This cost-aware router allows dynamically trading off accuracy for efficiency, enabling up to a 40\% reduction in expensive model calls without degrading answer quality. Similarly, \citet{shnitzer2023large} present a method to select the best model from a pool of pre-trained LLMs for each input by learning a “router” on many benchmark tasks. Without requiring labeled examples from the new target task, their approach uses existing datasets to train input-based model selectors, which consistently outperform always using the single best LLM for all queries. 

\subsubsection{Model Cascade Learning}
\citet{nie2024online} introduce an online cascade-learning framework where lightweight models are incrementally trained to imitate a powerful LLM’s decisions on a data stream, deferring to the LLM only when necessary. They cast cascade construction as an imitation-learning problem with theoretical no-regret guarantees, achieving LLM-level accuracy while cutting inference cost by up to 90\% and maintaining robustness to distribution shifts over time. \citet{chen2023frugalgpt} outline strategies for reducing LLM usage cost and present \emph{FrugalGPT}, a cascade approach that learns to route queries through combinations of smaller or larger LLMs to balance cost and performance. Their experiments show that an adaptive use of multiple models can match the accuracy of the strongest individual LLM (e.g., GPT-4) with up to 98\% cost savings. It can also slightly exceed GPT-4’s accuracy at equal cost, highlighting the benefit of cascades that allocate queries to the most appropriate model for each input. 

\subsubsection{Confidence Calibration in LLMs}

\citet{NEURIPS2023_1f09e1ee} analyze the classical strategy of confidence-based deferral in model cascades, wherein a model hands off to a stronger model if its confidence is below a threshold, to determine when this simple strategy succeeds or breaks down. They derive the optimal deferral policy in theory and show that naïve confidence thresholds perform well in general but can fail when later models are specialists (only reliable on certain inputs), when there is label noise, or under distribution shift – scenarios where more sophisticated deferral criteria yield better performance. \citet{geng2023survey} provide a comprehensive survey of methods for confidence estimation and calibration in LLM outputs. They review recent techniques to quantify uncertainty in large language model predictions, discuss challenges unique to LLMs, and highlight advancements that improve alignment between a model’s reported confidence and its actual accuracy across tasks. \citet{azaria2023internal} find evidence that an LLM’s internal activations encode whether or not it is producing a truthful answer, even when the model’s output is incorrect or fabricated. By training a classifier on the model’s hidden state (without fine-tuning the LLM itself), they can often detect when the model is “lying” or unsure, suggesting that large models internally recognize their mistakes or uncertainty despite outwardly confident responses. Similarly, \citet{liu2024uncertainty} propose a supervised approach to LLM uncertainty quantification that leverages labeled examples and the model’s hidden representations to predict the correctness of its answers. They show that incorporating features from the model’s internal layers yields significantly improved uncertainty estimates and calibration across diverse tasks, with these gains transferring robustly to new domains. Notably, their method is easy to implement and can be adapted to different levels of model access (black-box vs. white-box), making it widely applicable.

\subsubsection{Confidence Verbalization in LLMs}
\citet{lin2022teaching} demonstrate that GPT-3 can be fine-tuned to output a calibrated verbal confidence (e.g., ``I’m 90\% sure'') along with each answer. This model’s stated confidence levels align well with its true correctness likelihood and remain fairly well-calibrated even under distribution shift, marking the first instance of an LLM explicitly expressing useful uncertainty estimates in natural language. \citet{xiong2024can} thoroughly evaluate black-box methods for eliciting an LLM’s self-reported confidence through prompting and answer sampling. They find that current LLMs tend to verbalize overly high confidence (mirroring human overconfidence), but that carefully designed prompts, consistency checks across multiple sampled answers, and improved aggregation strategies can mitigate this issue. Moreover, larger models generally show better calibration and an improved ability to predict their own failures, though room for further improvement remains in making their expressed uncertainty truly reliable. \citet{mielke2022reducing} examine whether a conversational agent’s expressed certainty corresponds to its actual knowledge, showing that off-the-shelf dialogue models are poorly “linguistically calibrated.” They demonstrate that a model’s likelihood of giving a correct answer can be estimated via an auxiliary model and used as a control signal to adjust the agent’s responses. The resulting dialogue agent exhibits far less overconfident language when it is likely to be wrong, improving transparency about uncertainty in its answers. Finally, \citet{mahaut2024factual} assess the reliability of various methods to estimate an LLM’s \emph{factual confidence} – the probability that its answer is correct – under both in-domain and paraphrased inputs. Through a rigorous evaluation on QA and fact-checking tasks, they conclude that the most trustworthy confidence scores come from model-introspective approaches (e.g., a trained probe on hidden states), albeit at the cost of requiring full model access and training data. They also highlight that an LLM’s confidence can be unstable under meaning-preserving input variations (paraphrases), underscoring the need for more robust and stable confidence estimation techniques for factual correctness.

\subsection{Model Access Levels} In Figure~\ref{fig:model_access}, we show a schematic overview of different model access levels discussed in Section \ref{sec:related-word}.

\begin{figure*}[ht]
    \centering
    
    \begin{tikzpicture}[node distance=0.75cm]

\node[draw, ultra thick, minimum width=\linewidth, minimum height=4cm, anchor=north west] (border_black) at (current page.north west) {};
\node[anchor=north west, draw, ultra thick, fill=black, text=white] (title) at (border_black.north west) {Black box};

\node[below=of title, yshift=-1mm] (input) {Input};
\node[right=of input, draw, thick] (preprocess) {Preprocess};
\node[right=of preprocess, draw, thick, fill=black, text=white] (model_s) {$\mathcal{M}_S$};
\node[right=of model_s] (output_s) {Output$_S$};
\node[right=of output_s, draw, thick] (postprocess) {Postprocess};
\node[right=of postprocess, draw, thick] (deferral) {Deferral};

\node[below=of model_s, draw, thick, fill=black, text=white] (model_l) {$\mathcal{M}_L$};
\node[right=of model_l] (output_l) {Output$_L$};

% Add dashed horizontal line to separate the top and bottom flows
\draw[dotted, thick] ($(border_black.north west) + (0, -2.25cm)$) -- ($(border_black.north east) + (0,-2.25cm)$);
\node[anchor=north west, yshift=1.25cm, xshift=-1.75cm] (title) at (border_black.south east) {Remote};
\node[anchor=north west, yshift=-1.25cm, xshift=-1.75cm] (title) at (border_black.north east) {Local};


\draw[->, thick] (input) -- (preprocess);
\draw[->, thick] (preprocess) -- (model_s);
\draw[->, thick] (model_s) -- (output_s);
\draw[->, thick] (output_s) -- (postprocess);
\draw[->, thick] (postprocess) -- (deferral);

\draw[->, thick] (input) |- (model_l);
\draw[->, thick] (model_l) -- (output_l);

\draw[->, ultra thick] (deferral) |- node[anchor=south east] {Yes} (output_l);

\draw[->, ultra thick] (deferral) |- node[anchor=north east] {No} ++(0, 1) -| (output_s);

\end{tikzpicture}
    \vskip10pt
    \begin{tikzpicture}[]

\node[draw, ultra thick, minimum width=\linewidth, minimum height=4cm, anchor=north west] (border_black) at (current page.north west) {};
\node[anchor=north west, draw, ultra thick, fill=gray, text=white] (title) at (border_black.north west) {Gray box};

\node[below=of title, yshift=3mm] (input) {Input};
\node[right=of input, draw, thick, fill=gray, text=white] (model_s) {$\mathcal{M}_S$};
\node[right=of model_s] (logits) {Logits};
\node[right=of logits, draw, thick] (decode) {Decode};
\node[right=of decode] (output_s) {Output$_S$};
\node[right=of output_s, draw, thick] (deferral) {Deferral};

\node[below=of model_s, draw, thick, fill=black, text=white] (model_l) {$\mathcal{M}_L$};
\node[below=of output_s] (output_l) {Output$_L$};

% Add dashed horizontal line to separate the top and bottom flows
\draw[dotted, thick] ($(border_black.north west) + (0, -2.25cm)$) -- ($(border_black.north east) + (0,-2.25cm)$);

\node[anchor=north west, yshift=1.25cm, xshift=-1.75cm] (title) at (border_black.south east) {Remote};
\node[anchor=north west, yshift=-1.25cm, xshift=-1.75cm] (title) at (border_black.north east) {Local};


\draw[->, thick] (input) -- (model_s);
\draw[->, thick] (model_s) -- (logits);
\draw[->, thick] (logits) -- (decode);
\draw[->, thick] (decode) -- (output_s);
\draw[->, ultra thick] (deferral) -- node[anchor=south] {No} (output_s);

\draw[->, thick] (input) |- (model_l);
\draw[->, thick] (model_l) -- (output_l);

\draw[->, ultra thick] (deferral) |- node[anchor=south east] {Yes} (output_l);

\draw[->, thick] (logits) |- ++(0, 1) -| (deferral);

\end{tikzpicture}

\vskip10pt
    \begin{tikzpicture}[node distance=0.65cm]

\node[draw, ultra thick, minimum width=\linewidth, minimum height=4cm, anchor=north west] (border_black) at (current page.north west) {};
\node[anchor=north west, draw, ultra thick] (title) at (border_black.north west) {White box};

\node[below=of title, yshift=-2mm] (input) {Input};
\node[right=of input, draw, thick] (model_s) {$\mathcal{M}_S$};
\node[right=of model_s, draw] (tuning) {Tuning};
\node[right=of tuning] (logits) {Logits};
\node[right=of logits, draw, thick] (decode) {Decode};
\node[right=of decode] (output_s) {Output$_S$};
\node[right=of output_s, draw, thick] (deferral) {Deferral};

\node[below=of model_s, draw, thick, fill=black, text=white] (model_l) {$\mathcal{M}_L$};
\node[below=of output_s] (output_l) {Output$_L$};

\draw[->, thick, dashed] (input) |- (model_l);
\draw[->, thick, dashed] (input) -- (model_s);
\draw[->, thick, dashed] (model_s) -- (tuning);
\draw[->, thick, dashed] (tuning) |- ++(0, 1) -| (model_s);
% \draw[->, thick] (model_l) -- ++(0, 1) -| (tuning);
\draw[->, thick, dashed] (model_l) -| (tuning);

\draw[->, thick] (input) to [out=300, in=150] (model_l);
\draw[->, thick] (input) to [out=30, in=150] (model_s);
\draw[->, thick] (model_s) to [out=30, in=150] (logits);
\draw[->, thick] (model_l) to [out=15, in=180] (output_l);


\draw[->, thick] (logits) -- (decode);
\draw[->, thick] (decode) -- (output_s);
\draw[->, ultra thick] (deferral) -- node[anchor=south, yshift=5pt] {No} (output_s);
\draw[->, thick] (logits) |- ++(0, 1) -| (deferral);
\draw[->, ultra thick] (deferral) |- node[anchor=south east] {Yes} (output_l);

\draw[dotted, thick] ($(border_black.north west) + (0, -2.25cm)$) -- ($(border_black.north east) + (0,-2.25cm)$);
\node[anchor=north west, yshift=1.25cm, xshift=-1.75cm] (title) at (border_black.south east) {Remote};
\node[anchor=north west, yshift=-1.25cm, xshift=-1.75cm] (title) at (border_black.north east) {Local};

\end{tikzpicture}
    \caption[An overview of different uncertainty quantification strategies depending on model access level.]{\textbf{An overview of different uncertainty quantification strategies depending on model access level}.}
    \label{fig:model_access}
\end{figure*}

\subsection{Ideal Deferral Curve}
\label{app:ideal_deferral}

We present the functional form of the \emph{ideal deferral} curve, denoted
\(\mathrm{acc}_{\mathrm{ideal}}(r)\), for a small (student) model \(\mathcal{M}_S\) and a large (teacher) model \(\mathcal{M}_L\). Recall that \(r \in [0,1]\) denotes the deferral ratio, i.e., the fraction of inputs that \(\mathcal{M}_S\) “defers” to \(\mathcal{M}_L\). Let $p_s = \text{acc}(\mathcal{M}_S)$, and $p_l = \text{acc}(\mathcal{M}_L)$ with \(0 \le p_s \le p_l \le 1\). Our goal is to describe the maximum achievable joint accuracy if exactly a fraction \(r\) of the data is deferred to the large model.

\paragraph{Intuition and Setup}
Since \(\mathcal{M}_S\) achieves accuracy \(p_s\), it misclassifies a fraction \((1 - p_s)\) of the inputs. In an \emph{ideal} scenario, we defer exactly those inputs that \(\mathcal{M}_S\) is going to misclassify. Because \(\mathcal{M}_L\) is more accurate (\(p_l \ge p_s\)) every example misclassified by \(\mathcal{M}_S\) benefits from being passed to \(\mathcal{M}_L\).

\begin{itemize}
    \item \textbf{Case 1:} \(r \le (1 - p_s)\).\\
    We can use our entire deferral “budget” \(r\) to cover only those inputs \(\mathcal{M}_S\) would get wrong. Hence, deferring a fraction \(r\) of the data (all from \(\mathcal{M}_S\)'s mistakes) raises the overall accuracy by substituting \(\mathcal{M}_S\)'s errors with \(\mathcal{M}_L\)'s accuracy \(p_l\) on that fraction.
    \item \textbf{Case 2:} \(r > (1 - p_s)\).\\
    We have enough capacity to defer \emph{all} of \(\mathcal{M}_S\)'s mistakes, so the joint accuracy saturates at \(p_l\). Deferring \emph{additional} examples (which \(\mathcal{M}_S\) would have classified correctly) will not improve the overall accuracy beyond \(p_l\).
\end{itemize}

\paragraph{Piecewise Functional Form}
Thus, the \emph{ideal deferral} curve can be expressed as:
\begin{equation}
\mathrm{acc}_{\mathrm{ideal}}(r) \;=\;
\begin{cases}
p_s + \dfrac{p_l - p_s}{\,1 - p_s\,} \; r,
& \quad 0 \;\le\; r \;\le\; (1 - p_s), \\[1em]
p_l,
& \quad (1 - p_s) \;<\; r \;\le\; 1.
\end{cases}
\end{equation}
When \(0 \le r \le (1 - p_s)\), the overall accuracy grows linearly from \(\mathrm{acc}_{\mathrm{ideal}}(0) = p_s\) to \(\mathrm{acc}_{\mathrm{ideal}}(1-p_s) = p_l\). Past \(r = (1 - p_s)\), it remains constant at \(p_l\). 

Figure~\ref{fig:metrics_illustration} (b) in the main chapter plots this ideal deferral curve (green line). It serves as an upper bound on how effective any real deferral strategy can be. In contrast, a purely random deferral strategy produces a linear interpolation (the red line), which is strictly below the ideal curve for most \(r\). Consequently, the difference
\(\mathrm{acc}_{\mathrm{ideal}}(r) - \mathrm{acc}_{\mathrm{rand}}(r)\)
represents the \emph{maximum possible} gain one can achieve by carefully selecting which examples to defer rather than choosing them at random.

\paragraph{Summary} We summarize the key take-aways below:
\begin{itemize}
    \item \textbf{Ideal Deferral Routes All Mistakes:} Only the inputs misclassified by \(\mathcal{M}_S\) get deferred, guaranteeing the highest possible joint accuracy at each deferral level \(r\).
    \item \textbf{Piecewise Definition:} Accuracy increases linearly from \(p_s\) to \(p_l\) over the interval \(r \in [0,\, (1 - p_s)]\), then remains at \(p_l\).
    \item \textbf{Upper Bound on Realized Deferral:} No actual strategy can exceed this ideal curve, as it assumes perfect knowledge of which specific inputs \(\mathcal{M}_S\) would misclassify.
\end{itemize}



\section{Additional Experimental Details}

\subsection{CNN Used in Image Classification Experiments}

Below we include a representation of the \texttt{SmallCNN} model used as \smallmodel in image classification experiments discussed in Section \ref{sec:class_exp}:

\begin{lstlisting}[]
SmallCNN(
  (features): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=10, bias=True)
  )
)
\end{lstlisting}

\subsection{Reduce Confidence and Answer ``N'' Baselines}
\label{app:uncertainty_appendix}

In addition to the baseline model in Section \ref{sec:lang_exp} (i.e., a model that was not fine-tuned with our specialized \(\mathcal{L}_{\text{def}}\) loss but from which we still compute predictive entropy as a deferral signal), we also examine two additional methods aimed at eliciting uncertainty from the model directly via prompt modifications. Both methods are \textit{black box} approaches that only rely on a query interface to the model via prompt injection, and we provide their implementation details below.

\paragraph{Reduce Confidence.}
In this setting, we modify the original prompt \(\mathbf{x}\) by appending an additional instruction \(\mathbf{x}'\) that encourages the model to respond with lower confidence when it is uncertain: $\mathbf{x} \;\leftarrow\; \mathbf{x} \;\big\vert\; \mathbf{x}'$.
For instance, the instruction we add is:
\[
    \mathbf{x}' = \texttt{``Respond with low confidence if you are uncertain.''}
\]
We treat this appended text as a hint to the model to self-regulate its confidence when producing an answer. This is similar in spirit to other black box approaches such as confidence quantification, rejection awareness, remote model notice, and self-critiquing. Although \citet{xiong2024can} show that large language models can express aspects of their confidence via prompting, our experiments indicate that simply prompting the model to express lower confidence does not reliably improve the separation of correct versus incorrect predictions, nor does it offer advantages in a deferral setting. These findings are in line with those reported in \cite{kadavath2022language}.

\paragraph{Answer ``N.''}
\looseness=-1
We also consider an alternate prompt modification, in which the appended instruction is:
\[
    \mathbf{x}' = \texttt{``Respond with `N' if you are uncertain.''}
\]
This approach explicitly instructs the model to produce a special ``N'' token to indicate uncertainty or lack of confidence. The intuition is that by introducing a designated ``uncertain'' response, one might isolate uncertain cases for deferral. However, our results in Section \ref{sec:lang_exp} similarly show that the model’s ability to follow this instruction is inconsistent and does not substantially improve performance as a deferral model. The model often remains overconfident and fails to produce ``N'' in cases where it is in fact incorrect.

\subsection{Additional metrics}
\label{app:add_metrics}

In addition to the metrics outlined in Section~\ref{sec:experiments}, we also consider the \textbf{Area Under the Receiver Operating Characteristic Curve} (AUROC) ($s_\text{AUROC}$). The AUROC quantifies the model's ability to discriminate between correctly and incorrectly classified data points by evaluating the trade-off between the True Positive Rate (TPR) and the False Positive Rate (FPR) across various confidence thresholds $\tau$. Formally, given the confidence sets $\mathcal{C}_\text{corr}$ and $\mathcal{C}_\text{incorr}$, the AUROC is defined as
    \begin{equation}
        s_\text{AUROC} = \int_{0}^{1} \text{TPR}(\tau) \, \mathrm{d}\text{FPR}(\tau),
    \end{equation}
    where for each threshold $\tau \in [0,1]$ we compute $\text{TPR}(\tau) = \frac{|\{c \in \mathcal{C}_\text{corr} \mid c \geq \tau\}|}{|\mathcal{C}_\text{corr}|}$ and $\text{FPR}(\tau) = \frac{|\{c \in \mathcal{C}_\text{incorr} \mid c \geq \tau\}|}{|\mathcal{C}_\text{incorr}|}$. Note that $s_\text{AUROC} = 1$ indicates perfect separability and  $s_\text{AUROC} = 0.5$ corresponds to a random guessing baseline.

\subsection{Factuality Scoring}
\label{app:fac_scoring}

Factuality scoring with Gemini for a reference caption $r$ and a candidate caption $c$ is computed as follows:
\begin{enumerate}
    \item \textbf{Compute the log-likelihoods.} Let $\ell_{\text{Same}}(c, r)$ be the log-likelihood that the model outputs ``Same'' for a given candidate caption $c$ and reference $r$, and let $\ell_{\text{Diff}}(c, r)$ be the log-likelihood that the model outputs ``Different''.

    \item \textbf{Apply softmax.} To convert these log-likelihoods into probabilities, we exponentiate and normalize:
    \[
        p(\text{Same} \mid c, r) = \frac{\exp\bigl(\ell_{\text{Same}}(c, r)\bigr)}
        {\exp\bigl(\ell_{\text{Same}}(c, r)\bigr) + \exp\bigl(\ell_{\text{Diff}}(c, r)\bigr)},
    \]
    \[
        p(\text{Diff} \mid c, r) = \frac{\exp\bigl(\ell_{\text{Diff}}(c, r)\bigr)}
        {\exp\bigl(\ell_{\text{Same}}(c, r)\bigr) + \exp\bigl(\ell_{\text{Diff}}(c, r)\bigr)}.
    \]

    \item \textbf{Interpret the probability.} The value $p(\text{Same} \mid c, r)$ is then taken as the factual alignment score, expressing how confidently the model believes the candidate caption is factually aligned with the reference.
\end{enumerate}

\subsection{Additional Experimental Results}
\label{app:additional_exp_class}

In this section, we provide additional experimental results further supporting our findings reported for image classification experiments in Section \ref{sec:class_exp}. In particular, we show ROC curves in Figure \ref{fig:roc_class} and distributional overlap in Figure \ref{fig:dist_overlap_class}, both demonstrating that \loss increases the separation of correct/incorrect confidence scores. Similarly, the deferral curves in Figure \ref{fig:deferral_class} clearly show that \loss successfully pushed the realized deferral (black line) closer to the ideal one (marked with dashed upper line). Lastly, we report the joint accuracy of \smallmodel across varying $\alpha$ parameter in Figure \ref{fig:joint_acc_clss}. As discussed in Section \ref{sec:experiments}, we observe that \smallmodel's accuracy generally decreases with $\alpha \rightarrow 0$.

% Image

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/gatekeeper/rocs.pdf}
    % \vspace{-20pt}
    \caption[ROC curves for image classification experiments.]{\textbf{ROC curves for image classification experiments}. Each figure shows the ROC curves for each of the datasets considered in Section \ref{sec:class_exp}. We observe that \loss consistently increases separation of correct and incorrect confidence scores across varying $\alpha$ (colored curves) compared to the baseline (denoted with black dashed line).}
    \label{fig:roc_class}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/gatekeeper/deferrals_2.pdf}
    % \vspace{-20pt}
    \caption[Joint accuracy across different levels of $\alpha$.]{\textbf{Joint accuracy across different levels of $\alpha$}. For varying fixed deferral ratios, we observe that the accuracy of \smallmodel generally decreases as $\alpha \rightarrow 0$.}
    \label{fig:joint_acc_clss}
\end{figure*}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/gatekeeper/dist.pdf}
    % \vspace{-20pt}
    \caption[Distributional overlap for image classification experiments.]{\textbf{Distributional overlap for image classification experiments}. Left-most column shows the results obtained using the untuned baseline, while the remaining columns correspond to the results obtained using \loss with decreasing $\alpha$ values. Rows correspond to the datasets considered in Section \ref{sec:class_exp}. We see that \loss increases separation of correct and incorrect confidence scores compared to the baseline.}
    \label{fig:dist_overlap_class}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/gatekeeper/deferrals.pdf}
    % \vspace{-20pt}
    \caption[Deferral curves for image classification experiments.]{\textbf{Deferral curves for image classification experiments}. Left-most column shows the results obtained using the untuned baseline, while the remaining columns correspond to the results obtained using \loss with decreasing $\alpha$ values. Rows correspond to the datasets considered in Section \ref{sec:class_exp} The results show that \loss brings the realized deferral (black line) closer to the ideal deferral (dashed upper line).}
    \label{fig:deferral_class}
\end{figure*}

% Language

% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figs/gatekeeper/rocs_l.pdf}
%     \caption{Caption}
%     \label{fig:my_label}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figs/gatekeeper/deferrals_l.pdf}
%     \caption{Caption}
%     \label{fig:my_label}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figs/gatekeeper/dist_l.pdf}
%     \caption{Caption}
%     \label{fig:my_label}
% \end{figure*}

% Image Language


% \clearpage


