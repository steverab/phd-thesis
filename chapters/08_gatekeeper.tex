\newcommand{\bigmodel}{$\mathcal{M}_L$\xspace}
\newcommand{\smallmodel}{$\mathcal{M}_S$\xspace}

\newcommand{\loss}{\emph{Gatekeeper}\xspace}


\chapter{Gatekeeper: Improved Model Cascades Through Confidence Tuning}
\label{ch:gatekeeper}

% \paperref{\bibentry{rabanser2025gatekeeper}}

\begin{paperref}
\normalfont
The contents of this chapter consist of research and results taken from: \emph{\bibentry{rabanser2025gatekeeper}}
\end{paperref}

\section*{Summary}

Large-scale machine learning models deliver strong performance across a wide range of tasks but come with significant computational and resource constraints. To mitigate these challenges, local smaller models are often deployed alongside larger models, relying on routing and deferral mechanisms to offload complex tasks. However, existing approaches inadequately balance the capabilities of these models, often resulting in unnecessary deferrals or sub-optimal resource usage. In this work we introduce a novel loss function called Gatekeeper for calibrating smaller models in cascade setups. Our approach fine-tunes the smaller model to confidently handle tasks it can perform correctly while deferring complex tasks to the larger model. Moreover, it incorporates a mechanism for managing the trade-off between model performance and deferral accuracy, and is broadly applicable across various tasks and domains without any architectural changes. We evaluated our method on encoder-only, decoder-only, and encoder-decoder architectures.  Experiments across image classification, language modeling, and vision-language tasks show that our approach substantially improves deferral performance.

\section{Introduction}

\begin{figure}[t]
\centering

\begin{subfigure}[b]{0.66\linewidth}
  \centering
  \resizebox{\linewidth}{!}{\input{figs/gatekeeper/cascade_schema}}
  \caption{Cascading setup}
  \label{fig:cascade_schema}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\linewidth}
  \centering
  \includegraphics[width=\linewidth]{figs/gatekeeper/deferral_example_intro.pdf}
  \caption{Performance trade-off}
  \label{fig:cascade_performance}
\end{subfigure}

\caption[\textbf{Overview of the cascading setup (left) and performance trade-off (right).}]{\textbf{Overview of the cascading setup (left) and performance trade-off (right)}. \emph{Left}: Cascading determines which inputs should be predicted by a small model \smallmodel or routed to a large model \bigmodel. \emph{Right}: Performance is measured as a trade-off between joint accuracy across \smallmodel and \bigmodel and deferral ratio. Ideal deferral strategies optimize this trade-off and push the realized deferral curve closer to the ideal deferral depicted in (d). (a) depicts full deferral; (b) depicts no deferral; and (c) depicts excessive deferral of requests that could have been correctly handled by \smallmodel.}
\label{fig:deferral_example_intro}
\end{figure}


In recent years, large-scale machine learning models such as Gemini~\citep{team2023gemini}, GPT-4~\citep{achiam2023gpt} or Claude~\citep{antropicmodels} have gained significant traction due to their remarkable ability to address a wide array of tasks. These tasks range from natural language understanding and generation, including machine translation, summarization, and conversational agents, to computer vision applications like image recognition, object detection, and image captioning. The versatility and high performance of these expansive models make them invaluable tools across diverse domains, including healthcare~\citep{llm_healthcare}, finance~\citep{llm_finance}, education~\citep{llm_education}, and entertainment~\citep{llm_games}.

Deploying and operating such large models presents significant challenges in terms of latency, memory, compute and storage \citep{MLSYS2023_c4be71ab}. Optimizing inference costs is an active research area which includes both techniques for reducing the size of the existing large model such as model compression ~\citep{hoefler2021sparsity}, model pruning~\citep{ma2023llmpruner, pruning_survey} and distillation~\citep{knowledgedistilation_survey}, and those aiming to leverage a sequence of models such as speculative decoding~\citep{leviathan2023fast} and model cascades~\citep{dohan2022language}. However, due to scaling laws showing that the performance of a Large Language Model (LLM) increases with its size \citep{kaplan2020scaling}, the latter category of methods leveraging a sequence of models is currently a more promising direction to lower inference costs without sacrificing the capabilities of large models. %Thus, this work contributes to the advancement of these techniques.


Both speculative decoding and model cascading  rely on the existence of a large performant model \textbf{\bigmodel} and a small model \textbf{\smallmodel} that is cheap, fast, and less accurate. Speculative decoding leverages \textbf{\smallmodel} for generating a set of draft tokens that are then validated by \textbf{\bigmodel} in parallel, a technique successfully deployed in industry applications \citep{blogpost}. In contrast, model cascades leverage a deferral rule for selecting the most suitable model to process a given request (see Figure~\ref{fig:deferral_example_intro} left). While the success of the speculative decoding necessitates a highly performant \textbf{\smallmodel} to generate quality draft tokens, model cascades allow the deployment of a less capable \textbf{\smallmodel} by invoking \textbf{\bigmodel} only for inference requests outside the small model's scope. In this work, we contribute to the advancement of the model cascades. % which we explain next.  

Model cascades achieve efficient deferral by optimizing for two objectives: compute budget and joint accuracy. We illustrate their trade-off on the example shown in Figure~\ref{fig:deferral_example_intro} (right). Assume we have \textit{x} inference requests and a small model \textbf{\smallmodel} that only requires \textit{20\%} of the compute budget of the large model \textbf{\bigmodel}. There are three worst case scenarios: (a) the small model \textbf{\smallmodel} defers all requests to \textbf{\bigmodel} and the system achieves the best joint accuracy (equal to the accuracy of \textbf{\bigmodel}) but the worst compute budget (\textit{1.2x}) since all requests are run on both models; (b) \textbf{\smallmodel} never sends a request to \textbf{\bigmodel}, resulting in the smallest compute budget (\textit{0.2x}) but also the lowest joint accuracy (equal to the accuracy of \textbf{\smallmodel}); (c) \textbf{\smallmodel} only sends requests that it could have answered correctly to \textbf{\bigmodel}, requiring an increased compute budget compared to (b) but still resulting in the lowest joint accuracy (equal to the accuracy of \textbf{\smallmodel}). On the other hand, an ideal case is the scenario (d) where the small model \textbf{\smallmodel} only sends requests for which it would be incorrect, requiring a compute budget between \textit{0.2-1x} but resulting in the optimal joint accuracy given that budget. We call the approximation of the ideal case the \textit{deferral performance}.

In this chapter we address the following research question: 

\begin{center}
\textbf{How can we optimize model cascades to maximize deferral performance?} 
\end{center}


In other words, we focus on designing effective model cascades by making the small model more aware of what it does not know. We achieve this by introducing a \textit{general-purpose} loss function, called \loss, that calibrates the small model’s confidence in its predictions. By fine-tuning \textbf{\smallmodel} to output high confidence for correct predictions and low confidence for incorrect ones, we enhance the reliability of its uncertainty estimates and facilitate learning of common tasks, thereby directly improving the deferral performance.  Crucially, \loss includes an inherent mechanism for managing the trade-off between model performance and deferral accuracy that can be applied to an arbitrary architecture, making our work directly applicable to Vision-Language Models (VLMs). 

We demonstrate the efficacy of the \loss loss across various model architectures, including encoder-only vision models for image classification, decoder-only LMs for closed- and open-form text generation, and encoder-decoder setups for VL tasks for open set classification and captioning. Our main results show that models trained with \loss outperform an untuned baseline by a factor of 0.72x/2x on CIFAR-100/TinyImagenet and 7x/10x on ARC-e/c, respectively, in terms of deferral performance. This advancement paves the way for more scalable and efficient deployment strategies, leveraging the strengths of both local and large-scale models to deliver high-quality results in real-time applications.


\section{Related Work} \label{sec:related-word}

\begin{contriback}
This section was written with Nathalie Rauschmayr.
\end{contriback}


\sloppy
Our proposed method improves model cascades through uncertainty-aware finetuning. Next, we describe related work for both research areas.

\textbf{Model Cascades:} 
% From the CAT paper
A cascade consists of a series of models and a deferral rule which determines the appropriate model given an input request. The concept of model cascades has first been proposed by \citet{990517}, where it is used to accelerate object detection models. Cascades have been extensively studied for classification-based computer vision \citep{Wang2017IDKCF, pmlr-v31-trapeznikov13a, Bolukbasi2017AdaptiveNN, NEURIPS2023_1f09e1ee} and in models for natural language processing \citep{dohan2022language, mamou2022tangobertreducinginferencecost, varshney-baral-2022-model}. 

% Post-hoc methods
\looseness=-1
Cascades are particularly promising in the context of generative models such as LLMs and VLMs since they can significantly reduce inference costs. In contrast to speculative decoding~\citep{leviathan2023fast}, they aim to invoke the large model only for difficult examples. However, the two approached can also be combined. While~\citet{chen2024cascade} combine the deferral logic with speculative decoding to generate initial tokens using larger models and later tokens using a smaller model, the majority of research on model cascades has focused on using pre-trained LLMs with a post-hoc deferral logic~\citep{NEURIPS2022_bc8f76d9, NEURIPS2023_1f09e1ee, yue2024large}. \citet{kolawole2024agreementbasedcascadingefficientinference} use agreement across multiple models to make deferral decisions, while~\citet{gupta2024languagemodelcascadestokenlevel} present a method to learn a deferral rule based on quantiles of per-token log probabilities. 

%, but both approaches can also be combined \cite{narasimhan2024fastercascadesspeculativedecoding}. \citet{chen2024cascade} also combines the deferral logic with speculative decoding to generate initial tokens using larger models and later tokens using a smaller model. The majority of research on model cascades has focused on using pre-trained LLMs with a post-hoc deferral logic \cite{NEURIPS2022_bc8f76d9}, \cite{NEURIPS2023_1f09e1ee}, \cite{yue2024large}. \cite{kolawole2024agreementbasedcascadingefficientinference} uses agreement across multiple models to make deferral decisions.  \cite{gupta2024languagemodelcascadestokenlevel} presents a method to learn a deferral rule based on quantiles of per-token log probabilities. 

% Fine-tuning methods
Model cascades can be further improved through training and fine-tuning. \citet{wang2024cascadeawaretraininglanguagemodels} train the small model only on easier examples by masking tokens for which large and small model are incorrect. \citet{Enomoro_Eda_2021} extend the training objective of image classification models with confidence calibration. In contrast to previous research, our work extends cascades to VLMs and improves overall inference performance by making smaller models less confident when they are incorrect.

\textbf{Uncertainty-Aware Models:} Extensive research has been conducted in the field of uncertainty quantification in deep learning and we refer to~\citet{abdar2021review} for a detailed survey. While many methods have been proposed for classification-based models, measuring uncertainty for generative models is still an active area of research.
Based on the assumed level of access to model internals, existing methods can be summarized into three main categories:

\emph{Black box} methods operate solely via the model’s query interface by injecting tailored instructions into prompts. These modify the prompt $\mathbf{x}$ by appending instructions $\mathbf{x}'$ for the model to respond less confidently: $\mathbf{x} \leftarrow \mathbf{x} | \mathbf{x}'$. Related methods are confidence quantification~\citep{shrivastava2023llamas}, rejection and remote model awareness~\citep{kadavath2022language}, and self-critiquing~\citep{gou2023critic}. \citet{xiong2024can} show that LLMs can express their confidence through prompting and sampling strategies and their experiments indicate that these models tend to be overconfident.
 
\emph{Gray box} approaches employ confidence-based strategies centered on post-processing the model’s logits. Many uncertainty techniques such as ensembling~\citep{lakshminarayanan2017simple} and Bayesian methods~\citep{blundell2015weight}) are not scalable. Related techniques are  max confidence~\citep{hendrycks2016baseline}, predictive entropy, and confidence reduction prompting. \citet{malinin2021uncertainty} uses token-entropy as a measure of uncertainty in auto-regressive models and~\citet{kuhn2023semantic} leverages linguistic invariances via semantic entropy.
 
\emph{White box} methods utilize uncertainty-aware fine-tuning in order to produce more accurately calibrated models. \citet{chuang2024learningrouteconfidencetokens} introduces Self-REF, a framework which leverages confidence tokens during fine-tuning to improve performance in downstream routing. \citet{krishnan2024enhancingtrustlargelanguage} proposes an uncertainty-aware causal language modeling loss function, which captures the trade-off between predictive accuracy and uncertainty calibration. In contrast to previous work, our method aims to calibrate the model in a way such that correctly generated tokens are assigned low predictive uncertainty and incorrectly generated tokens are assigned high predictive uncertainty. We consider the uncertainty-aware model in the context of cascade inference system, where it helps to improve overall performance. Furthermore, we present in-depth ablation studies to quantify the trade-offs of the proposed loss function.
%Our work is closely related to \cite{krishnan2024enhancingtrustlargelanguage} which fine-tunes the model in a way such that correctly generated tokens are assigned low predictive uncertainty denoting high confidence and incorrectly generated tokens are assigned high predictive uncertainty. The key differences are: 1) we consider the uncertainty-aware model in the context of cascade inference system, where it helps to improve overall performance. 2) We evaluate this fine-tuning strategy across a variety of models. Contrarily to~\citet{krishnan2024enhancingtrustlargelanguage}, we observe that it can deteriorate the baseline performance at the expense of the model better understanding when it is incorrect. 3) Our method assigns different weights to the loss-terms and we ran in-depth ablation studies to better understand the trade-offs between the both.

% \emph{\textcolor{red}{@Nathalie: Can you see if the below paragraphs on Black and Gray box approaches can be integrated into what you wrote above?}}

% \paragraph{Black box}

% \begin{itemize}
%     \item These are prompt injection approaches that only rely on a query interface to the model. They modify the prompt $\mathbf{x}$ by appending instructions $\mathbf{x}'$ for the model to respond less confidently: $\mathbf{x} \leftarrow \mathbf{x} | \mathbf{x}'$. If \smallmodel does not abide the prompting format, it will be deferred to \bigmodel. 
%     \item \emph{Confidence Quantification}: Instruct the model to respond with a confidence score: $\mathbf{x}' = $"Respond with a confidence score in range [0,1]."
%     \item \emph{Rejection Awareness}: Instruct the model to respond with low confidence if the model is uncertain: $\mathbf{x}' = $"Answer with "reject" if you are uncertain." In closed form evaluation setups (e.g., multiple choice answer questions) models can also be instructed to respond with a non-of-the-above option. $\mathbf{x}' = $"Answer with "none-of-the-above" if you are uncertain."
%     \item \emph{Remote Model Notice}: Inform the model that a more capable model is available and can take over if it is uncertain: $\mathbf{x}' = $"Answer with "defer" if you are uncertain to defer the decision to a more capable model."
%     \item \emph{Self-Critiquing}: Feed a generation back into the model to assess whether the model is certain that this generation is correct.
%     \item \emph{\textcolor{red}{Anything else?}}
% \end{itemize}

% \paragraph{Gray box}

% \begin{itemize}
%     \item These are confidence-based techniques that leverage post-processing model logits. Many uncertainty techniques (ensembling, bayesian methods) are not scalable.
%     \item \emph{Max Confidence}: Extract the maximum confidence value. In closed form evaluation setups, just consider the relevant response token; in open evaluations setups, summarize this score across the full sequence. Summarization can happen with different norms or averaging.
%     \item \emph{Predictive Entropy}: Similar to max confidence, but consider the full conditional distribution over tokens, not just the maximum response.
%     \item \emph{Confidence Reduction Prompting}: Instruct the model to respond with low confidence if the model is uncertain: $\mathbf{x}' = $"Respond with low confidence if you are uncertain.". Then, evaluate the 
%     \item \emph{\textcolor{red}{Anything else?}}
% \end{itemize}

% \emph{\textcolor{red}{@Nathalie: rauschmayr: What about techniques that generate multiple model predictions given the same or similar input prompts?}}

\section{Method}

% \begin{itemize}
%     \item Setup: We have a big model \bigmodel and a small model \smallmodel.
%     \item This model can be a classifier or a sequence model (language model or vision language model).
%     \item Deferral can be done either via using signals of the small local model or by using a separate router. In our case, we focus on the small model scenario as this is typically cheaper. 
%     \item We assume that the small model is strictly less capable than the big model. All mistakes made by the big model are also made by the small model but some mistakes made by the small model would not be made by the big model. This is a realistic assumption in practice where we observe that bigger models are more capable than smaller models.
%     \item Depending on the level of access to the small model, different techniques can be employed to determine a deferral signal:
%     \begin{itemize}
%         \item \textbf{Black Box} (input and output only): Utilize prompt injection techniques to encourage the model to express uncertainty in its responses, either qualitatively or quantitatively.
%         \item \textbf{Gray Box} (input, logits, and outputs): Analyze the logits or sample various generations from the model to assess uncertainty.
%         \item \textbf{White Box} (input, full model internals, and outputs): Fine-tune the model to incorporate deferral or rejection mechanisms directly.
%     \end{itemize}
%     \item Non-Applicable Approaches: Approaches relying on the interaction with a larger model (as is common in the language modeling domain) are not applicable in our setup. Since the goal is to determine when to defer to a larger model, calling a larger model to help with a deferral decision effectively already corresponds to a deferral. Such approaches include but are not limited to LLM collaboration and semantic entropy.
% \end{itemize}

\subsection{Overview \& Setup}

%\looseness=-1
Our framework consists of a large, highly capable model \textbf{\bigmodel} and a smaller, resource-efficient model \textbf{\smallmodel}. We assume that $S \in \mathbb{N}$ and $L \in \mathbb{N}$ represent the parameter count of each model with $S \ll L$. Both models can either function as classifiers (i.e., $\mathcal{M}: \mathbb{R}^D \rightarrow [C]$ with $C$ denoting the classes), or (multi-modal) sequence models (i.e., $\mathcal{M}: \mathbb{R}^D \rightarrow [V]^{T}$ where $V$ is the vocabulary and $T$ is the sequence length). %Note that our empirical results in Section~\ref{sec:experiments} include experiments on all of these model classes. 
We include experiments on all of these model classes in Section~\ref{sec:experiments}. Furthermore, we do not require a shared model family to be deployed on both \smallmodel and \bigmodel; for example, \smallmodel could be a custom convolutional neural network optimized for efficient inference and \bigmodel a vision transformer~\citep{dosovitskiy2020image}. The primary objective is to design a deferral mechanism that enables \smallmodel to decide when to return its predictions without the assistance of \bigmodel and when to instead defer to it. We assume that \bigmodel is either outside of our control (e.g., an API endpoint) or too costly to modify, and that only \smallmodel is subject to adaptation.

\looseness=-1
Deferral decisions are made using signals derived from the small model \smallmodel as this approach is typically more cost-effective than employing a separate routing mechanism~\citep{teerapittayanon2016branchynet}. Approaches that involve querying the large model \bigmodel to assist in making deferral decisions at test time are excluded from our setup. Such methods --- common in domains like LLMs --- are counterproductive to our goal since querying \bigmodel inherently constitutes a deferral. Examples of these inapplicable methods include collaborative LLM frameworks~\citep{mielke2022reducing} and techniques that rely on semantic entropy for uncertainty estimation~\citep{kuhn2023semantic}. As part of our setup, we assume that \smallmodel is strictly less capable than \bigmodel --- a realistic scenario in practice supported by scaling laws~\citep{kaplan2020scaling}. Under this assumption, mistakes made by \bigmodel are also made by \smallmodel; however, \smallmodel may make additional errors that \bigmodel would avoid. This reflects the general observation that larger models tend to outperform smaller models across a wide range of tasks.

\looseness=-1
As discussed in Section~\ref{sec:related-word}, the choice of deferral strategy often depends on the level of access available to \smallmodel. We assume white box access with full access to \smallmodel's internals. As such, deferral mechanisms can be directly integrated into the model's architecture and parameters. This involves fine-tuning \smallmodel to predict deferral decisions or to incorporate rejection mechanisms within its predictive process. Our work falls into this category as it proposes a new loss function to fine-tune \smallmodel. 

% \looseness=-1
Our goal is to train a small model that can effectively distinguish between correct and incorrect predictions. While many past works have considered the question of whether it is possible to find proxy measures for correctness, the central question we ask is: 

\begin{center}
\textbf{Can we \emph{optimize} the small model \smallmodel to separate correct from incorrect predictions?}
\end{center}

We show that this is indeed achievable through a carefully designed fine-tuning stage that does not require any architectural modifications. This ensures that the ability to separate correct from incorrect decisions is integrated seamlessly into \smallmodel's existing structure.


\subsection{Confidence-Tuning for Deferral}

\begin{figure}
    \centering
    \resizebox{\linewidth}{!}{
    \input{figs/gatekeeper/loss}
    }
    % \vspace{-15pt}
    \caption[Overview of \loss.]{\textbf{Overview of \loss}: We want correctly predicted samples maintain their current prediction by ensuring that cross entropy is decreased (top, green). At the same time, we want incorrectly predicted samples to yield a uniform confidence across all classes, leading to a low overall confidence score (bottom, red).}
    \label{fig:opt_goal}
\end{figure}

\textbf{Stage 1: Standard Training.} We begin with a \smallmodel that has already been trained on the tasks it is intended to perform upon deployment. However, due to its limited capacity, \smallmodel cannot achieve the performance levels of \bigmodel. Importantly, we make no assumptions about the training process of \smallmodel—whether it was trained from scratch without supervision from an external model or through a distillation approach.

\sloppy
\textbf{Stage 2: Correctness-Aware Finetuning with \loss.} Next, we introduce a correctness-aware loss, dubbed \loss, to fine-tune \smallmodel for improved confidence calibration. Specifically, the model is trained to make correct predictions with high confidence while reducing the confidence of incorrect predictions (see Figure~\ref{fig:opt_goal}). This loss can either rely on true labels or utilize the outputs of \bigmodel with soft probabilities as targets. 


For a standard classification model, the calibration loss is defined as the following hybrid loss
\begin{align}
\mathcal{L} &= \alpha \mathcal{L}_\text{corr} + (1 - \alpha) \mathcal{L}_\text{incorr} \\
\mathcal{L}_\text{corr} &= \frac{1}{N} \sum_{i=1}^{N} \mathds{1}\{ y_i = \hat{y}_i \} \text{CE}(p_i(\mathbf{x}_i), y_i) \\
\mathcal{L}_\text{incorr} &= \frac{1}{N} \sum_{i=1}^{N} \mathds{1}\{ y_i \neq \hat{y}_i \} \text{KL}\left(p_i(\mathbf{x}_i) \parallel \mathcal{U}\right)
\end{align}
where  \( y_i \) and \( \hat{y}_i \) are the true and predicted label, respectively, \( p_i \) is the predicted probability distribution over classes, \( \mathcal{U} \) represents the uniform distribution over all classes, \( N \) denotes the number samples in the current batch, \( \alpha \in (0, 1) \) is a tunable hyperparameter controlling the emphasis between correct and incorrect predictions, and the cross-entropy function and KL divergence are defined as \( \text{CE}(p, y) = -\sum_{c} y_c \log p_c \) and \( \text{KL}(p \parallel q) = \sum_{c} p_c \log ( \frac{p_c}{q_c}) \), respectively. We note that a similar loss has previously been proposed in Outlier Exposure (OE)~\citep{hendrycks2018deep} for out-of-distribution (OOD) sample detection. Here, the goal is to make sure that OOD examples are assigned low confidence scores by tuning the confidence on an auxiliary outlier dataset. However, to the best of our knowledge, this idea has not previously been used to improve deferral performance of a smaller model in a cascading chain.

We emphasize that the trade-off parameter $\alpha$ plays a critical role as part of this optimization setup as it directly influences model utility and deferral performance. A lower value of \(\alpha\) emphasizes reducing confidence in incorrect predictions by pushing them closer to the uniform distribution, making the model more cautious in regions where it may make mistakes. Conversely, a higher value of \(\alpha\) encourages the model to increase its confidence on correct predictions, sharpening its decision boundaries and enhancing accuracy where it is already performing well. Thus, \(\alpha\) serves as a crucial hyperparameter that balances the trade-off between improving calibration by mitigating overconfidence in errors and reinforcing confidence in accurate classifications. By appropriately tuning \(\alpha\), practitioners can control the model’s behavior to achieve a desired balance between reliability in uncertain regions and decisiveness in confident predictions, tailored to the specific requirements of their application.

We further generalize this loss to token-based models (e.g., LMs and VLMs), formulated as % by applying the same principle at token level
\begin{align}
    \mathcal{L}_\text{corr} & = \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \mathds{1}\{ y_{i,t} = \hat{y}_{i,t} \} \text{CE}(p_{i,t}(\mathbf{x}_i), y_{i,t}) \\
    \mathcal{L}_\text{incorr} & = \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \mathds{1}\{ y_{i,t} \neq \hat{y}_{i,t} \} \text{KL}\left(p_{i,t}(\mathbf{x}_i) \parallel \mathcal{U}\right)
\end{align}
\normalsize
where \( y_{i,t} \) and \( \hat{y}_{i,t} \) denote the true and predicted tokens at position \( t \) for sample \( i \), \( p_{i,t} \) is the predicted token distribution at position \( t \) for sample \( i \), and \( T \) is the sequence length for the token-based model. The token-level loss ensures that correct token predictions are made confidently while incorrect tokens are assigned smaller confidences.

\sloppy
\textbf{Stage 3: Confidence Computation \& Thresholding.} After fine-tuning \smallmodel with \loss, we apply standard confidence- and entropy-based techniques for model uncertainty to obtain a deferral signal. We use the selective prediction framework to determine whether a query point~$\mathbf{x} \in \mathbb{R}^D$ should be accepted by \smallmodel or routed to \bigmodel. Selective prediction alters the model inference stage by introducing a deferral state through a \textit{gating mechanism}~\citep{yaniv2010riskcoveragecurve}. At its core, this mechanism relies on a deferral function $g:\mathbb{R}^D \rightarrow \mathbb{R}$ which determines if \smallmodel should output a prediction for a sample~$\mathbf{x}$ or defer to \bigmodel. Given a targeted acceptance threshold $\tau$, the resulting predictive model can be summarized as:
\begin{equation}
\label{eq:deferral}
    (\mathcal{M}_S,\mathcal{M}_L,g)(\mathbf{x}) = \begin{cases}
  \mathcal{M}_S(\mathbf{x})  & g(\mathbf{x}) \geq \tau \\
  \mathcal{M}_L(\mathbf{x}) & \text{otherwise.}
\end{cases}
\end{equation}

\emph{Classification Models (Max Softmax).} Let \(\mathcal{M}_S\) produce a categorical distribution
\(\{p(y=c \mid \mathbf{x})\}_{c=1}^C\) over \(C\) classes. 
Then we define the gating function as
\begin{align}
g_{\text{CL}}(\mathbf{x}) \;=\; \max_{1 \,\le\, c \,\le\, C}\;p\bigl(y = c \,\big\vert\, \mathbf{x}\bigr).
\end{align}

\emph{Token-based Models (Negative Predictive Entropy).} 
Let \(\mathcal{M}_S\) produce a sequence of categorical distributions 
\(\{p(y_t = c \mid \mathbf{x})\}_{c=1}^C\) for each token index \(t \in T\). Then we define the gating function as
\begin{equation}
\footnotesize
g_{\text{NENT}}(\mathbf{x}) 
= \; \frac{1}{T} \sum_{t=1}^{T} \sum_{c=1}^{C} 
    p\bigl(y_t = c \,\big\vert\, \mathbf{x}\bigr)\,\log p\bigl(y_t = c \,\big\vert\, \mathbf{x}\bigr),
\end{equation}
where \(y_t \in [C]\) is the predicted token at time step \(t\), \(p(y_t=c \mid \mathbf{x})\) is the (conditional) probability of token \(k\) at step \(t\), and \(T\) is the total number of token positions for the sequence. Across both model classes, higher values of either $g_{\text{CL}}$ or $g_{\text{NENT}}$ indicate higher confidence in the predicted class or sequence generation, respectively.

% \begin{itemize}
%     \item \emph{Calibration loss}: Tune the model such that correct token predictions are made confidently and incorrect token predictions are made less confidently. Uses true labels right now but could use \bigmodel with soft probabilities.
%     \begin{align*}
%     \mathcal{L} = & \alpha \cdot \frac{1}{N_=} \sum_{i=1}^{N_=} \mathds{1}\{ y_i = \hat{y}_i \} \cdot \text{CrossEntropy}(p_i, y_i) \\ & + (1 - \alpha) \cdot \frac{1}{N_{\neq}} \sum_{i=1}^{N_{\neq}} \mathds{1}\{ y_i \neq \hat{y}_i \} \cdot \text{KL}\left(p_i \parallel \mathcal{U}\right)
% \end{align*}
% \item $\alpha \in (0,1)$ controls where to place emphasis during loss minimization. A low value emphasizes pushing incorrect predictions closer to a uniform distribution. Larger values encourge the model to mostly focus on sharpening its prediction ability on already correctly predicted examples.
% \end{itemize}


    
    % \item \emph{Deferral token} Add a rejection token that models rejection.
    % \item \emph{Deferral head} Train another head that is supposed to reject uncertain tokens, leading to a deferral.
    % \item \emph{\textcolor{red}{Anything else?}}
    % \item Open Questions: On which examples do we want the model to defer? Should the model figure this out by itself or should we specify such a preference during fine-tuning? Is this static? Can the model change this preference over time? What are common properties of misclassified / wrongly predicted data points? How do we assess example difficulty?



    \section{Experiments}
\label{sec:experiments}

In this section, we detail the experiments used to evaluate the effectiveness of \loss across three distinct model architectures: encoder-only classification models, decoder-only language models, and encoder-decoder vision–language models. Each setup involves a cascade where a smaller model can defer uncertain inputs to a larger, more capable model.

%The chosen datasets—CIFAR-10/100, Food-101, and ImageNet—provide a comprehensive spectrum of classification tasks, ranging from simple object recognition to fine-grained and large-scale image classification challenges.

\sloppy
\subsection{Encoder-only Setup (Classification Models)}
\label{sec:class_exp}

We comprehensively assess the performance of our deferral strategies across different model architectures and task types, starting with image classification. We train both a large model and a small model on the following datasets: CIFAR-10/100~\citep{krizhevsky2009learning}, Food-101~\citep{bossard14}, and TinyImageNet200~\citep{Le2015TinyIV}. For both CIFAR datasets we use a ResNet-18~\citep{he2016deep} as \bigmodel and a custom CNN as \smallmodel. For Food-101 and TinyImageNet200 we instead use a ResNet-50~\citep{he2016deep} as \bigmodel and a Mobilenet V3 Small \citep{howard2019searching} as \smallmodel, where the latter is trained using knowledge distillation from the big model.

\paragraph{Evaluation Metrics.}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/gatekeeper/metrics_example_class.pdf}
    % \vspace{-20pt}
    \caption[Performance metrics overview.]{%\textbf{Deferral Performance}: 
    \textbf{Performance metrics overview}: \textbf{(a)}~Distributional Overlap~$s_o$: the densities of confidence scores for correctly (green) and incorrectly classified (red) samples, with the overlap area shaded in blue. Smaller values are better ($\downarrow$).
    \textbf{(b)}~Deferral Performance~$s_d$: how joint accuracy between \smallmodel and \bigmodel varies with deferral ratio, showing random (red), ideal (green), and realized (black) deferral strategies. 
    The blue region shows the realized performance gain, the hatched portion represents the range of useful deferral functions, and the green region indicates the potential headroom over the realized deferral. Larger values are better ($\uparrow$).}
    \label{fig:metrics_illustration}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/gatekeeper/summary_metrics.pdf}
    % \vspace{-20pt}
    \caption[Performance on image classification tasks.]{\textbf{Performance on image classification tasks}. We observe that lower levels of $\alpha$ lead to decreased distributional overlap between correct/incorrect predictions (left), increased deferral performance (center) and generally decreased performance over the full data distribution (right). These results support our conclusion that the small model \smallmodel learns to refocus on easier subsets of the distribution while understanding more reliably when it should abstain and defer to the large model \bigmodel.}
    \label{fig:image_class_results}
\end{figure*}

We measure the performance of our confidence tuning method and the resulting deferral function~$g(\cdot)$ using the following performance metrics (see example in Figure~\ref{fig:metrics_illustration} for a visual overview):

\begin{enumerate}
    \item The \textbf{Distributional Overlap of Confidences of Correct and Incorrect Predictions} $s_o$ is defined as the integral of the minimum of the probability density functions (PDFs) of confidence scores for correctly classified samples, $\hat{p}_{\text{corr}}(c)$, and incorrectly classified samples, $\hat{p}_{\text{incorr}}(c)$ (see Figure \ref{fig:metrics_illustration}a). Formally, given the confidence sets $\mathcal{C}_\text{corr}$ and $\mathcal{C}_\text{incorr}$, the overlap $s_o$ is computed as
\begin{equation}
    s_o = \int_{0}^{1} \min\left\{ \hat{p}_{\text{corr}}(c),\ \hat{p}_{\text{incorr}}(c) \right\} \, \mathrm{d}c,
\end{equation}
where the PDFs are estimated using Kernel Density Estimation (KDE). If $s_o = 1$, then \smallmodel cannot distinguish the confidence distribution of correct and incorrect predictions; if $s_o = 0$, then \smallmodel can perfectly separate correct and incorrect predictions. Note that a related way of capturing the distributional separability is given by the Area Under the Receiver Operating Characteristic Curve (AUROC) which we discuss in Appendix~\ref{app:add_metrics}.

    
    \item \textbf{Deferral Performance $s_d$}:  
To formally quantify how well \smallmodel defers difficult inputs to \bigmodel, we examine the joint performance across all possible deferral ratios $r \in [0,1]$, where $r$ denotes the fraction of inputs sent to \bigmodel based on a particular threshold $\tau$ (recall Equation~\eqref{eq:deferral}). Figure~\ref{fig:metrics_illustration} b) illustrates how, as $r$ increases from $0$ to $1$, the overall (joint) accuracy $\text{acc}(r)$ increases from the accuracy of \smallmodel (yellow circle, no deferral) to the accuracy of \bigmodel (purple square, full deferral). Useful deferral models are constrained to operate between random deferral ($\mathrm{acc}_{\mathrm{rand}}$, red dotted line) and ideal deferral ($\mathrm{acc}_{\mathrm{ideal}}$, green dashed line). The ideal deferral $\mathrm{acc}_{\mathrm{ideal}}$ corresponds to the oracle solution that perfectly defers examples misclassified by \smallmodel and we discuss its exact functional form in Appendix~\ref{app:ideal_deferral}. We also define the realized deferral curve, $\mathrm{acc}_{\mathrm{real}}$, as the joint accuracy obtained under the learned deferral strategy $g(\cdot)$ employed by \smallmodel and \bigmodel. The deferral performance metric \( s_d \) is then given as:
\begin{equation}
\label{eq:deferral_performance}
\small
s_d = \frac{A_{\mathrm{perf}}}{A_{\mathrm{useful}}} = \frac{\int_{0}^{1} \left( \mathrm{acc}_{\mathrm{real}}(r) - \mathrm{acc}_{\mathrm{rand}}(r) \right) \, \mathrm{d}r}{\int_{0}^{1} \left( \mathrm{acc}_{\mathrm{ideal}}(r) - \mathrm{acc}_{\mathrm{rand}}(r) \right) \, \mathrm{d}r}.
\end{equation}
This ratio quantifies the fraction of the potential improvement over random deferral that has been realized by the achieved deferral strategy. Note that $s_d = 1$ indicates perfect deferral, matching the ideal strategy, while an $s_d = 0$ implies no improvement over random deferral.


% \begin{itemize}[leftmargin=1.12em]
%     \item \textbf{Random Deferral (red line):} A baseline strategy $A_{\mathrm{rand}}(r)$ where \smallmodel\ defers a random subset of inputs.
%     \item \textbf{Ideal Deferral (green line):} An \emph{oracle} $A_{\mathrm{ideal}}(r)$ that defers exactly those queries that \smallmodel\ would misclassify, thus achieving the highest possible accuracy at each $r$.
%     \item \textbf{Achieved Deferral (black curve):} Our learned or heuristic strategy $A_{\mathrm{achieved}}(r)$, which typically lies between the random and ideal deferral lines.
% \end{itemize}

% To compute the \emph{deferral performance} $s_d$, we integrate (or sum, in discrete form) the area under $A_{\mathrm{achieved}}(r)$ from $r=0$ to $r=1$, capturing the average joint accuracy as we sweep through all deferral ratios:
% \[
% s_d \;=\; \int_{0}^{1} A_{\mathrm{achieved}}(r)\,\mathrm{d}r.
% \]

% The figure highlights two additional areas of interest to better interpret $s_d$:
% \begin{enumerate}
%     \item \textbf{Headroom Area} (green region) is the space between random and ideal deferral. It signifies the maximum potential gain from selective deferral.
%     \item \textbf{Performance Area} (blue region) is the area under the black curve, indicating how much of that potential we actually exploit.
% \end{enumerate}
% The cross-hatched (overlapping) region—labeled “Useful Area”—shows the portion of the Headroom Area \emph{actually realized} by our strategy, and hence directly reflects how effectively \smallmodel\ identifies and defers its most uncertain inputs to \bigmodel.



% \item \textbf{Deferral Performance ($s_d$)}:  
% To evaluate how effectively the \smallmodel\ defers “difficult” inputs to the \bigmodel, we define the deferral performance metric \( s_d \) as the ratio of the \textbf{Performance Area} (blue region in \autoref{fig:deferral}(c)) to the \textbf{Useful Area} (cross-hatched region in \autoref{fig:deferral}(c)). Formally, the Performance Area is the integral of the \emph{Achieved Deferral} curve $A_{\mathrm{achieved}}(r)$ over all deferral ratios $r \in [0, 1]$:
% \[
% \text{Performance Area} = \int_{0}^{1} \left( A_{\mathrm{achieved}}(r) - A_{\mathrm{rand}}(r) \right) \, \mathrm{d}r,
% \]
% where $A_{\mathrm{rand}}(r)$ is the joint accuracy under random deferral. The \textbf{Useful Area} is the maximum possible improvement over random deferral, defined as the area between the \emph{Ideal Deferral} curve $A_{\mathrm{ideal}}(r)$ and the \emph{Random Deferral} curve $A_{\mathrm{rand}}(r)$:
% \[
% A_{\mathrm{useful}} = \int_{0}^{1} \left( d_{\mathrm{ideal}}(r) - d_{\mathrm{rand}}(r) \right) \, \mathrm{d}r.
% \]


    \item \textbf{Accuracy of the small model} $\text{acc}(\mathcal{M}_S)$: Finally, since \loss emphasizes patterns for distinguishing correct/incorrect examples, the model is no longer encouraged to minimize the classification loss over the full population. As a result, improving on the correct/incorrect separation task can lead to changes in utility over the full data distribution. Hence, practically useful deferral methods need to balance both deferral performance and the accuracy of \smallmodel.
\end{enumerate}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figs/gatekeeper/metrics_example_class.pdf}
%     \vspace{-20pt}
%     \caption{%\textbf{Deferral Performance}: 
%     \textbf{Performance metrics overview}: \textbf{(a)}~Distributional Overlap~($s_o$): the densities of confidence scores for correctly classified (green) and incorrectly classified (red) instances, with the overlap area shaded in blue.
%     \textbf{(b)}~AUROC~($s_{\mathrm{AUROC}}$): the ROC curve (black) compared against the random baseline (dotted), highlighting the area under the curve in blue.
%     \textbf{(c)}~Deferral Performance~($s_d$): how joint accuracy between \smallmodel and \bigmodel varies with deferral ratio, showing random (red), ideal (green), and realized (black) deferral strategies. 
%     The blue region denotes the realized performance gain, the cross-hatched portion represents the range of useful deferral functions, and the green region indicates the potential headroom over the realized deferral.}
%     \label{fig:metrics_illustration}
% \end{figure}


% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figs/gatekeeper/metrics_example_class.pdf}
%     \caption{\textbf{Performance metrics example}.}
%     \label{fig:l_results}
% \end{figure*}

% \begin{table*}
% \caption{Performance Comparison for dist\_overlap with Percentage Improvements}
% \label{tab:performance_dist_overlap}
% \centering
% \small
% \begin{tabular}{lcccccc}
% \toprule
%  & 1.0 & 0.9 & 0.7 & 0.5 & 0.3 & 0.1 \\
% \midrule
% \textbf{tinyimagenet200} & 0.584 & 0.448 (-23.21\%) & 0.443 (-24.12\%) & 0.421 (-27.84\%) & 0.321 (-45.04\%) & 0.219 (-62.41\%) \\
% \textbf{food101} & 0.391 & 0.308 (-21.14\%) & 0.328 (-16.06\%) & 0.308 (-21.1\%) & 0.281 (-28.06\%) & 0.196 (-49.88\%) \\
% \textbf{cifar10} & 0.481 & 0.422 (-12.17\%) & 0.456 (-5.25\%) & 0.402 (-16.38\%) & 0.4 (-16.78\%) & 0.396 (-17.74\%) \\
% \textbf{cifar100} & 0.564 & 0.49 (-13.1\%) & 0.466 (-17.36\%) & 0.431 (-23.51\%) & 0.365 (-35.38\%) & 0.239 (-57.68\%) \\
% \bottomrule
% \end{tabular}
% \end{table*}

% \begin{table*}
% \caption{Performance Comparison for deferral\_performance with Percentage Improvements}
% \label{tab:performance_deferral_performance}
% \centering
% \small
% \begin{tabular}{lcccccc}
% \toprule
%  & 1.0 & 0.9 & 0.7 & 0.5 & 0.3 & 0.1 \\
% \midrule
% \textbf{tinyimagenet200} & 0.366 & 0.668 (82.43\%) & 0.719 (96.42\%) & 0.757 (106.91\%) & 0.844 (130.66\%) & 0.84 (129.56\%) \\
% \textbf{food101} & 0.858 & 0.742 (-13.53\%) & 0.796 (-7.2\%) & 0.859 (0.12\%) & 0.879 (2.46\%) & 0.905 (5.52\%) \\
% \textbf{cifar10} & 0.684 & 0.73 (6.78\%) & 0.72 (5.25\%) & 0.791 (15.64\%) & 0.793 (15.93\%) & 0.79 (15.46\%) \\
% \textbf{cifar100} & 0.64 & 0.734 (14.59\%) & 0.747 (16.72\%) & 0.781 (22.01\%) & 0.844 (31.82\%) & 0.899 (40.45\%) \\
% \bottomrule
% \end{tabular}
% \end{table*}

% \begin{table*}
% \caption{Performance Comparison for auroc with Percentage Improvements}
% \label{tab:performance_auroc}
% \centering
% \small
% \begin{tabular}{lcccccc}
% \toprule
%  & 1.0 & 0.9 & 0.7 & 0.5 & 0.3 & 0.1 \\
% \midrule
% \textbf{tinyimagenet200} & 0.774 & 0.827 (6.82\%) & 0.848 (9.5\%) & 0.852 (10.1\%) & 0.894 (15.48\%) & 0.91 (17.49\%) \\
% \textbf{food101} & 0.873 & 0.886 (1.52\%) & 0.89 (2.0\%) & 0.91 (4.27\%) & 0.927 (6.24\%) & 0.956 (9.51\%) \\
% \textbf{cifar10} & 0.83 & 0.852 (2.74\%) & 0.849 (2.29\%) & 0.881 (6.14\%) & 0.882 (6.25\%) & 0.883 (6.48\%) \\
% \textbf{cifar100} & 0.794 & 0.829 (4.48\%) & 0.846 (6.56\%) & 0.865 (9.01\%) & 0.887 (11.71\%) & 0.934 (17.69\%) \\
% \bottomrule
% \end{tabular}
% \end{table*}

% \begin{table*}
% \caption{Performance Comparison for small\_accuracy with Percentage Improvements}
% \label{tab:performance_small_accuracy}
% \centering
% \small
% \begin{tabular}{lcccccc}
% \toprule
%  & 1.0 & 0.9 & 0.7 & 0.5 & 0.3 & 0.1 \\
% \midrule
% \textbf{tinyimagenet200} & 0.215 & 0.214 (-0.14\%) & 0.208 (-3.08\%) & 0.194 (-9.41\%) & 0.16 (-25.58\%) & 0.106 (-50.37\%) \\
% \textbf{food101} & 0.733 & 0.774 (5.68\%) & 0.769 (4.92\%) & 0.741 (1.18\%) & 0.699 (-4.51\%) & 0.564 (-22.99\%) \\
% \textbf{cifar10} & 0.772 & 0.778 (0.73\%) & 0.781 (1.14\%) & 0.741 (-4.05\%) & 0.735 (-4.76\%) & 0.712 (-7.86\%) \\
% \textbf{cifar100} & 0.449 & 0.431 (-4.14\%) & 0.398 (-11.44\%) & 0.368 (-18.2\%) & 0.303 (-32.67\%) & 0.194 (-56.92\%) \\
% \bottomrule
% \end{tabular}
% \end{table*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figs/gatekeeper/summary_metrics_vl_class.pdf}
%     \vspace{-20pt}
%     \caption{\textbf{Performance on VLM classification tasks}.}
%     \label{fig:vl_results}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figs/gatekeeper/tradeoff_class.pdf}
%     \includegraphics[width=\linewidth]{figs/gatekeeper/tradeoff_llm.pdf}
%     \includegraphics[width=\linewidth]{figs/gatekeeper/tradeoff_vlm.pdf}
%     \caption{\textbf{Performance tradeoffs}.}
%     \label{fig:tradeoffs}
% \end{figure*}

% \paragraph{Deferral Performance Metric} We illustrate the purpose of the deferral performance in Figure~\ref{fig:deferral_perf}. The deferral performance (shown in green) reflects how effectively a model can improve overall accuracy by selectively “handing off” hard cases to an expert. As the system increases the fraction of deferred instances, it can offload uncertain or ambiguous inputs, thereby allowing an expert to make those decisions. Meanwhile, the model focuses on the remaining “easier” instances that it can handle with greater confidence. The green region in the figure captures this collective gain over baseline performance—where the model alone would be less accurate—and highlights the extent to which a well-chosen deferral strategy can boost joint accuracy. By precisely identifying which instances to retain versus defer, the system maximizes the benefits of expert intervention without needlessly deferring straightforward tasks.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/gatekeeper/tradeoff_class_cut.pdf}
    % \vspace{-20pt}
    \caption[Performance trade-off between small model accuracy $\text{acc}(\mathcal{M}_S)$ and deferral performance $s_d$.]{\textbf{Performance trade-off between small model accuracy $\text{acc}(\mathcal{M}_S)$ and deferral performance $s_d$}. The baseline model obtained without fine-tuning using \loss is often the most accurate model over the full data distribution. With the introduction of \loss we can improve distinguishability of correct/incorrect predictions (left) as well as deferral (right) at the expense of model utility. Successful cascading solutions in practice need to balance both model accuracy and deferral performance.}
    \label{fig:tradeoffs}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/gatekeeper/summary_metrics_l.pdf}
    % \vspace{-20pt}
    \caption[Performance on language modeling tasks.]{\textbf{Performance on language modeling tasks}. Similar as Figure~\ref{fig:image_class_results}. In addition to a non-tuned baseline, we also add an uncertainty prompting baseline, an Answer ``N'' option, as well as the post-hoc confidence calibration method from~\citet{gupta2024languagemodelcascadestokenlevel}. We observe that \loss outperforms other methods at lower levels of $\alpha$.}
    \label{fig:l_results}
\end{figure*}

% \looseness=-1
\textbf{Results.} Our main results are shown in Figure~\ref{fig:image_class_results}. We report performance for both a baseline model (an instance of \smallmodel not trained with \loss) and small models trained with \loss at various $\alpha$ values. We also compare against~\citet{NEURIPS2022_bc8f76d9}, a common cascading baseline for supervised learning tasks. For all models, we compute deferral performance and correct/incorrect separation (center, left). The strongest performance occurs at low $\alpha$s, where the model pushes outputs of incorrect examples closer to uniform. However, this comes at a cost: the small model’s accuracy degrades at low $\alpha$s (right), highlighting that the model effectively “unlearns” performance on harder examples to focus on easier ones. At larger $\alpha$s, accuracy remains stable or improves slightly, as training emphasizes already well-predicted points. While the baseline from~\citet{NEURIPS2022_bc8f76d9} preserves model accuracy, it requires explicit estimation of the expert model’s correctness—necessitating either an architectural change (e.g., an added prediction head) or a separate prediction network. In contrast, \loss relies solely on confidence tuning of the small model, making it operationally easier to deploy. This approach not only simplifies implementation but also leads to improved deferral performance by producing more reliable uncertainty estimates.

This result highlights a critical trade-off which is directly controlled by $\alpha$: \emph{how strongly do we want to degrade model performance over the full data distribution in order to obtain a better deferral model?} We note that this compromise between raw model utility and deferral performance is not surprising and similar trade-offs exist in fairness~\citep{dutta2020there, yaghini2023learning} and privacy~\citep{abadi2016deep, rabanser2023training}. We study this trade-off explicitly in Figure~\ref{fig:tradeoffs} showing (i) a clear negative correlation between deferral performance and the small model's accuracy; and (ii) a clear positive correlation between the overlap of correct/incorrect confidences and the accuracy of \smallmodel. 
% Mention that this tradeoff exists in many other subfields (privacy <--> utility, privacy < ---> uncertainty). rabanser2023training, yaghini2023, etc. Show the results we got. 

% \paragraph{Encoder-only Setup (Classification Models)}
% For the encoder-only configuration, we focus on state-of-the-art image classification models that process input images to generate class predictions. 

%The primary architecture under investigation is the ResNet family, renowned for its residual connections that facilitate the training of deep networks by mitigating the vanishing gradient problem. Specifically, we implement ResNet-50 and ResNet-101 variants to serve as the larger models in our deferral framework. Additionally, to ensure a robust evaluation, we incorporate other leading architectures such as Vision Transformers (ViT) and ConvNeXt. ViT models leverage the transformer architecture's capability to capture global context through self-attention mechanisms, while ConvNeXt modernizes traditional convolutional networks by integrating transformer-inspired design elements, thereby enhancing both performance and efficiency.

% The deferral mechanism is applied by deploying smaller, resource-efficient models (e.g., MobileNetV3 or EfficientNet-B0) as the initial classifiers. These local models handle straightforward and high-confidence predictions, deferring only the uncertain or complex queries to the larger counterparts. This hierarchical approach aims to optimize computational resource utilization and reduce latency by minimizing the number of queries processed by the heavyweight models.

\subsection{Decoder-only Setup (Language Models)}
\label{sec:lang_exp}

In the decoder-only setup, we explore the application of LLMs. Our primary models of interest are the scalable LMs from the Gemma model class~\citep{team2024gemma}. We choose Gemma2B as \smallmodel and Gemma7B as \bigmodel with 2 billion and 7 billion parameters, respectively. Similar to the encoder-only setup, we employ smaller LMs as the initial classifiers to manage simpler next-token prediction tasks. The deferral strategy involves routing only those token sequences that exhibit high uncertainty --- as determined by high predictive entropy --- to the more powerful model \bigmodel.

Our experiments start by taking the instruction-tuned checkpoints of Gemma2B and Gemma7b and fine-tuning both models on the training split of a respective dataset to ensure that the model (i) performs well on the task; and (ii) is familiar with the desired response format. Note that this fine-tuning stage is performed using standard perplexity minimization. Then, we finetune \smallmodel with \loss using the same training split to decrease confidence on incorrect next-token predictions. Finally, we evaluate the model yielded by \loss on a validation/test split. The datasets we consider are ARC-e/c~\citep{clark2018think}, MMLU~\citep{hendrycks2020measuring}, and GSM8K~\citep{cobbe2021training}. We use the same evaluation metrics as previously used in Section~\ref{sec:class_exp}.

\textbf{Results.} We present our main results in Figure~\ref{fig:l_results}, comparing the baseline model’s deferral and correct/incorrect separation ability to our fine-tuned model across different $\alpha$s. We observe a similar trend as in the image classification setting: higher $\alpha$s maintain raw prediction performance closer to the baseline but offer limited gains in separation, while lower $\alpha$s improve deferral more substantially at the cost of overall accuracy. In addition to the baseline model (not fine-tuned with \loss), we include results from~\citet{gupta2024languagemodelcascadestokenlevel} (an extension of~\citet{NEURIPS2022_bc8f76d9} to token-based sequence models), as well as two uncertainty prompting baselines (described in Appendix~\ref{app:uncertainty_appendix}): (i) \emph{Reduce Confidence}, which appends instructions to encourage the model to lower confidence when uncertain; and (ii) \emph{Answer ``N''}, which instructs the model to respond with ``N'' if uncertain. We find that \loss outperforms~\citet{gupta2024languagemodelcascadestokenlevel} in terms of correct/incorrect sepration and deferral at the cost of overall utility. Consistent with prior findings from~\citet{kadavath2022language}, the prompting baselines do not reliably improve deferral.

%However, so far these results are more noisy and we will try to improve these before submission.

\begin{figure*}
\centering

\begin{subfigure}[b]{0.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/gatekeeper/summary_metrics_vl_class.pdf}
  \caption{Classification task}
  \label{fig:vl_results_class}
\end{subfigure}
\hfill
\vrule width 0.5pt
\hfill
\begin{subfigure}[b]{0.48\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/gatekeeper/summary_metrics_vl_gen.pdf}
  \caption{Captioning task}
  \label{fig:vl_results_gen}
\end{subfigure}

\caption[\textbf{Performance on VLM classification (left) and captioning tasks (right).}]{\textbf{Performance on VLM classification (left) and captioning tasks (right)}. Consistent with results in Figures~\ref{fig:image_class_results} and \ref{fig:l_results}, we see that smaller $\alpha$s lead to improved deferral performance in both classification and generation tasks.}
\label{fig:vl_results}
\end{figure*}

\subsection{Encoder-Decoder Setup (Vision-Language Models)}
Finally, we examine models that incorporate both visual and textual processing capabilities, making it ideal for tasks that require a comprehensive understanding of image content in conjunction with language generation. We consider the PaliGemma~\citep{steiner2024paligemma} model family which are encoder-decoder models designed to perform VL tasks such as image captioning, visual question answering, and image classification with descriptive outputs. In this setup, the encoder component processes the input images to extract rich feature representations, while the decoder generates corresponding textual classifications or descriptions. We use PaliGemma1B as \smallmodel and PaliGemma7B as \bigmodel. The deferral strategy involves deploying a smaller VLM to handle the majority of classification tasks, reserving the more resource intensive 7B model for instances where \smallmodel's predictive entropy falls below a predefined threshold.

Similarly to our experiments on LMs in Section~\ref{sec:lang_exp}, we employ two stages of fine-tuning. First, we take the instruction-tuned checkpoints of PaliGemma1B and PaliGemma7B and then fine-tune both models on the training split of a given dataset. Next, we fine-tune only \smallmodel using \loss before evaluating the model on a validation/test split of the dataset. The datasets we consider are two classification datasets (VQAv2~\citep{goyal2017making}, AI2D~\citep{hiippala2021ai2d}) and two captioning datasets (Cococap~\citep{lin2014microsoft}, Screen2Words~\citep{wang2021screen2words}). This allows us to evaluate \loss in both closed-form vision-language classification setups as well as open-form text generation.

% \paragraph{Evaluation Metrics}
% Across all experimental setups, we utilize standard evaluation metrics to quantify model performance and the effectiveness of the deferral strategies:

% \begin{itemize}
%     \item \textbf{Accuracy}: Measures the proportion of correctly classified instances out of the total number of instances.
    
%     \item \textbf{Precision, Recall, F1-Score}: Provide a more nuanced understanding of model performance, particularly in multi-class and imbalanced classification scenarios.
    
%     \item \textbf{Deferral Rate}: Calculates the percentage of queries deferred to the larger models based on uncertainty thresholds.
    
%     \item \textbf{Latency and Computational Cost}: Assesses the time and resources required to process queries, highlighting the efficiency gains from the deferral strategy.
    
%     \item \textbf{Confidence Calibration}: Evaluates how well the model's confidence scores align with actual prediction accuracy, ensuring reliable uncertainty estimates.
% \end{itemize}

% \paragraph{Implementation Details}
% All models are implemented using PyTorch, leveraging pre-trained weights where applicable to facilitate transfer learning and accelerate convergence. Training and evaluation are conducted on NVIDIA GPUs with ample memory to handle large-scale models and datasets. Hyperparameters such as learning rate, batch size, and threshold values for deferral are meticulously tuned through cross-validation to optimize performance across different architectures and datasets.

% In summary, our experimental setup meticulously integrates diverse model architectures with strategic deferral mechanisms, evaluated across a suite of benchmark datasets. This comprehensive approach enables a thorough investigation into the interplay between model size, architecture, and deferral strategies, ultimately aiming to enhance the efficiency and accuracy of image classification systems.

% \paragraph{BLEU Score}
% The Bilingual Evaluation Understudy (BLEU) score, denoted as \( s_{\text{BLEU}} \), is a widely adopted metric for evaluating the quality of machine-generated image captions by comparing them to one or more reference captions. BLEU assesses the precision of $n$-grams (typically up to 4-grams) in the candidate caption relative to the reference captions, while incorporating a brevity penalty to discourage overly short descriptions. Mathematically, the BLEU score is defined as:

% $$
% s_{\text{BLEU}} = \text{BP} \cdot \exp \left( \sum_{n=1}^{N} w_n \log p_n \right)
% $$

% In this equation, $\text{BP}$ represents the brevity penalty, which is calculated as:

% $$
% \text{BP} = 
% \begin{cases} 
%   1 & \text{if } c > r, \\
%   e^{(1 - r/c)} & \text{if } c \leq r,
% \end{cases}
% $$

% where $c$ is the length of the candidate caption and $r$ is the effective reference corpus length. The term $p_n$ denotes the modified $n$-gram precision, measuring the proportion of $n$-grams in the candidate that appear in the references. Each $n$-gram precision is weighted by $w_n$, typically set to $\frac{1}{N}$ to ensure equal weighting across all considered $n$-gram lengths. The combination of these components ensures that \( s_{\text{BLEU}} \) effectively captures both the accuracy and the completeness of the generated captions.

% \paragraph{CIDEr Score}
% The Consensus-based Image Description Evaluation (CIDEr) score, represented as \( s_{\text{CIDEr}} \), is designed to evaluate the similarity of generated image captions to a set of reference captions, emphasizing the importance of informative and less frequent $n$-grams through Term Frequency-Inverse Document Frequency (TF-IDF) weighting. The CIDEr score is calculated as:

% $$
% s_{\text{CIDEr}} = \frac{1}{|R|} \sum_{r \in R} \text{cos\_sim}(c, r)
% $$

% Here, $R$ represents the set of reference captions, and $c$ is the candidate caption. The cosine similarity between the TF-IDF weighted vectors of the candidate and each reference caption is computed to measure their similarity. This similarity is given by:

% $$
% \text{cos\_sim}(c, r) = \frac{\sum_{i=1}^{V} \text{TF-IDF}(c)_i \cdot \text{TF-IDF}(r)_i}{\sqrt{\sum_{i=1}^{V} (\text{TF-IDF}(c)_i)^2} \cdot \sqrt{\sum_{i=1}^{V} (\text{TF-IDF}(r)_i)^2}}
% $$

% In this formula, $V$ denotes the size of the vocabulary. The use of TF-IDF ensures that $n$-grams that are more unique and informative across the corpus contribute more significantly to \( s_{\text{CIDEr}} \), thereby providing a more nuanced evaluation of the caption quality. By averaging the cosine similarities across all reference captions, \( s_{\text{CIDEr}} \) captures the consensus among references, ensuring that the generated caption aligns well with multiple valid descriptions.

\textbf{Factuality Scoring.}
% For the classification tasks (i.e., VQAv2 and AI2D) we apply our analysis in the same way as in Section~\ref{sec:lang_exp}. However, for the captioning datasets we need to evaluate the quality of a caption generated by PaliGemma. To do that, we compute a factuality score which judges whether the caption is coherent with respect to a reference caption. Leveraging the capabilities of the Gemini Large Language Model~\citep{team2023gemini}, this factuality score evaluates the alignment between the generated caption and the ground truth by verifying the presence and correctness of factual information. The factuality score for a single data point $\mathbf{x}_i$ is defined as:
% \begin{equation}
%     s_{\text{Fac}}(\mathbf{x}_i) = \frac{1}{|E(\mathbf{x}_i)|} \sum_{e \in E(\mathbf{x}_i)} \texttt{fac\_score}(e, c(\mathbf{x}_i))
% \end{equation}
% In this equation, $E(\mathbf{x}_i)$ represents the set of factual elements (such as entities, attributes, and relationships) extracted from the reference captions, and $c(\mathbf{x}_i)$ is the candidate caption. The score $\texttt{fac\_score}(e,c(\mathbf{x}_i))$ computes the degree to which each factual element $e$ is accurately represented in $c(\mathbf{x}_i)$. This is achieved by prompting the Gemini LLM to evaluate the presence and correctness of each fact within the generated text. Specifically, the Gemini LLM analyzes the semantic and contextual consistency of each factual element, assigning a score between 0 and 1, where 1 indicates perfect factual alignment and 0 signifies a complete mismatch or absence. By aggregating these scores across all factual elements, \( s_{\text{Fac}} \) provides a quantitative measure of the caption's factual accuracy. The factuality over the full dataset is then given by $s_{\text{Fac}} = \frac{1}{N} \sum_{i=1}^{N} s_{\text{Fac}}(\mathbf{x}_i)$.
For classification tasks we apply our analysis in the same way as in Section~\ref{sec:lang_exp}. However, for captioning datasets we need to evaluate the quality of a caption generated by PaliGemma. To do that, we compute a factuality score which judges whether the generated caption is semantically coherent with respect to a reference caption using the Gemini LLM~\citep{team2023gemini}. Specifically, the Gemini LLM is prompted with an instruction of the form: \emph{``Are these captions semantically equivalent?''}, followed by both the candidate caption and the reference caption. The model then responds with either \emph{``Yes''} or \emph{``No''}. Finally, we compute the log-likelihood of each response and normalize it to a probability, reflecting the LLM's confidence in the captions being factually aligned. We detail this process in Appendix~\ref{app:fac_scoring} and denote the factuality score for input point $\mathbf{x}_i$ with candidate caption $\hat{\mathbf{y}}_i$ and ground truth caption $\mathbf{y}_i$ as $s_{\text{Fac}}(\hat{\mathbf{y}}_i, \mathbf{y}_i)$.


\textbf{Measuring Correlation Between Factuality and Negative Predictive Entropy.} Since the result of evaluating $s_{\text{Fac}}(\hat{\mathbf{y}}_i, \mathbf{y}_i)$ is no longer binary, our evaluation metrics which previously relied on accuracy cannot be used directly to evaluate deferral performance and the correct/incorrect entropy distribution separation. We address this issue by replacing the distributional overlap computation with the Pearson correlation $\rho(g_{\text{NENT}}(\mathbf{x}_i), s_{\text{Fac}}(\hat{\mathbf{y}}_i, \mathbf{y}_i))$ between the negative predictive entropy of a caption $g_{\text{NENT}}(\mathbf{x}_i)$ and its associated factuality score $s_{\text{Fac}}(\hat{\mathbf{y}}_i, \mathbf{y}_i))$. We also adapt our deferral performance metric from Equation~\eqref{eq:deferral_performance} to rely on factuality measures instead of accuracy. 

%, complementing other evaluation metrics like \( s_{\text{BLEU}} \) and \( s_{\text{CIDEr}} \) to ensure a comprehensive assessment of caption quality.

\textbf{Results.}  We document our results in Figure~\ref{fig:vl_results} where we compare the baseline model's deferral ability against our fine-tuned models at different $\alpha$s. For the classification results (Figure~\ref{fig:vl_results} left), we observe the same trends as outlined in our classification and language modeling experiments. For the captioning results (Figure~\ref{fig:vl_results_gen} right) we observe that \loss measurably increases the correlation between factuality and negative predictive entropy, yielding better deferral from \smallmodel to \bigmodel with decreasing $\alpha$. This demonstrates that our method does not just work on closed-form classification problems but also generalizes to open-form sequence generation tasks. Note that while we also benchmarked the prompting baselines from Section~\ref{sec:lang_exp} for these experiments, the PaliGemma model did not return any responses for these modified prompts (likely due to PaliGemma's rigid pretraining and prompting instructions~\citep{beyer2024paligemma}).

% \paragraph{Deferral Computation}
% To enhance the overall quality of generated image captions, we employ a **deferral strategy** based on the uncertainty score \( s_{\text{unc}} \). This strategy involves determining whether to accept a generated caption or defer it for further review based on a predefined uncertainty threshold \( \tau \). The deferral computation is formalized as follows:

% \textbf{Acceptance Decision:}
% For each generated caption \( c_i \) with an associated uncertainty score \( s_{\text{unc}_i} \), we define an acceptance decision \( a_i \) as:

% $$
% a_i =
% \begin{cases}
% 1 & \text{if } s_{\text{unc}_i} \leq \tau, \\
% 0 & \text{if } s_{\text{unc}_i} > \tau.
% \end{cases}
% $$

% Here, \( a_i = 1 \) indicates that the caption \( c_i \) is accepted for use, while \( a_i = 0 \) signifies that the caption is deferred for human review or alternative processing.

% \textbf{Deferral Rate:}
% The deferral rate \( \text{DR} \) is defined as the proportion of captions that are deferred based on their uncertainty scores:

% $$
% \text{DR} = \frac{1}{N} \sum_{i=1}^{N} (1 - a_i) = 1 - \frac{1}{N} \sum_{i=1}^{N} a_i
% $$

% where \( N \) is the total number of generated captions. A higher deferral rate implies that a larger proportion of captions are being deferred due to high uncertainty.

% \textbf{Performance on Accepted Captions:}
% The performance metrics on the accepted captions, denoted as \( P(\tau) \), are computed by averaging the evaluation scores over the set of accepted captions:

% $$
% P(\tau) = \frac{\sum_{i=1}^{N} a_i \cdot P_i}{\sum_{i=1}^{N} a_i}
% $$

% where \( P_i \) represents one or more performance metrics such as \( s_{\text{BLEU}} \), \( s_{\text{CIDEr}} \), or \( s_{\text{Fac}} \) for caption \( c_i \). This equation calculates the average performance of the captions that are accepted, providing insight into the quality of the captions retained after deferral.

% \textbf{Trade-off Analysis:}
% By varying the threshold \( \tau \), we can analyze the trade-off between the deferral rate \( \text{DR} \) and the performance \( P(\tau) \) of the accepted captions. Specifically, increasing \( \tau \) results in more captions being accepted (lower deferral rate) but may include captions with lower quality. Conversely, decreasing \( \tau \) increases the deferral rate, filtering out lower-quality captions and enhancing the average performance of the accepted set.

% This Deferral Rate vs. Performance on Accepted Captions analysis is visualized through a plot where the deferral rate is plotted on the x-axis against the performance metrics on the y-axis. The plot typically shows a positive correlation, indicating that higher deferral rates lead to improved performance on the captions that are accepted. This correlation validates the effectiveness of the uncertainty-based deferral strategy in maintaining high-quality outputs by selectively filtering out less reliable generations.



% \paragraph{Metrics}


% \begin{itemize}
%     \item Deferral rate / call rate of remote model.
%     \item AUROC of uncertainty score at local model (classification).
%     \item Correlation of uncertainty score with Cider/Bleu/Factuality (language generation).
%     \item Deferral vs overall accuracy / performance.
%     \item Correct/Incorrect distributional overlap.
%     \item When available: difficulty / uncertainty correlation.
% \end{itemize}

% What to focus on next?

% \begin{itemize}
%     \item Make sure all approaches are implemented?
%     \item More datasets?
% \end{itemize}

% Hopefully shows that our method is useful.

\section{Conclusion}

In this work we present a novel loss function called \loss for improving confidence calibration in a cascade between a small local and a larger remote model. Our loss is architecture and task agnostic, making it flexibly applicable across a wide range of applications. Our results demonstrate that our approach improves over standard confidence-based deferral rules and effectively leads the small model to unlearn how to handle complex queries in favor of easier ones. 

\textbf{Limitations} While our approach demonstrates promising results, there are a few notable constraints. First, we assume that only the smaller model can be tuned, while in some application the larger model might also adjustable for deferral. Second, our experiments primarily measure improvement over a single untuned baseline, potentially overlooking broader comparative insights. Third, we did not extensively evaluate across different model families in LLM and VLM settings (although we did so in classification experiments with ResNet vs.\ MobileNet). Finally, using a generative model (e.g., Gemini) to judge VLM captioning introduces the risk of erroneous assessments since LLMs are also imperfect oracles.