% Machine learning (ML) now underpins a wide range of critical applications, from healthcare diagnostics to autonomous vehicles. However, as models transition from controlled research environments to real-world contexts, their reliability comes under heightened scrutiny. This thesis tackles the challenge of making ML systems more robust and trustworthy by placing the concept of \emph{uncertainty} at the forefront. 

% We first explore how \emph{selective prediction} mechanisms—where a model abstains on inputs deemed too uncertain—can markedly enhance reliability without modifying the core model architecture. By monitoring training dynamics and calibrating confidence estimates, we show that selective prediction can mitigate high-risk errors across tasks such as image classification, regression, and time-series analysis. We also investigate how requirements like \emph{differential privacy} introduce additional noise into the learning process, potentially undermining conventional uncertainty quantification methods, and we propose refinements to maintain reliability under privacy constraints.

% Next, we develop theoretical bounds that dissect the trade-off between the fraction of inputs on which a model provides predictions (coverage) and the accuracy on those accepted inputs. These results illuminate the role of calibration and confidence distributions in driving selective prediction performance. Beyond these methodological contributions, we examine adversarial scenarios wherein dishonest institutions manipulate uncertainty to deny services, highlighting the critical need for verifiability in abstention mechanisms. Finally, we illustrate how smaller local models and larger specialized models can be orchestrated to reduce computational overhead by deferring only complex queries to the more powerful model.

% Overall, this thesis provides a comprehensive framework—spanning empirical techniques, theoretical insights, and adversarial considerations—to design, evaluate, and deploy ML models capable of reliable decision-making under uncertainty. 

\noindent Machine learning (ML) systems are increasingly deployed in high-stakes domains where reliability is paramount. As these systems move from research prototypes to real-world decision-makers, their ability to recognize and respond to uncertainty becomes critical. This thesis investigates how uncertainty estimation can drive safer and more trustworthy ML, focusing in particular on \emph{selective prediction}—a paradigm in which models abstain from making predictions when uncertainty is high.

We begin by demonstrating that signals from a model’s training trajectory can be used to build powerful uncertainty scores without modifying model architecture or loss functions. This checkpoint-based approach enables the model to flag unreliable predictions, improving performance across image, regression, and time-series benchmarks. Crucially, because this method only observes the training process, it remains compatible with differential privacy guarantees—a property we exploit to study how privacy constraints affect the quality of uncertainty estimates. Our findings reveal that many uncertainty quantification methods degrade under privacy due to the addition of noise.

% , prompting the development of refined evaluation metrics that better capture performance in such settings.

This limitation motivates a principled investigation into the core factors that govern selective prediction performance. Through both theoretical analysis and empirical validation, we identify five distinct sources of error that account for suboptimal performance. This decomposition, in turn, enables actionable recommendations for improving selective prediction systems in practice. However, this decomposition assumes that uncertainty only arises from legitimate error sources, an assumption we challenge by introducing and defending against adversarial manipulations that exploit confidence-based abstention to mask discriminatory behavior. Finally, we explore how uncertainty can guide decisions in resource-constrained settings, where small models handle easy inputs and defer complex queries to larger models. We show how to calibrate and train such model cascades to ensure both efficiency and reliability.

Together, these contributions chart a path toward more trustworthy ML by rigorously studying how uncertainty can be estimated, evaluated, and safeguarded—ultimately enabling models that not only predict accurately, but also know when to say “I do not know.”
