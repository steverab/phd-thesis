% Machine learning (ML) now underpins a wide range of critical applications, from healthcare diagnostics to autonomous vehicles. However, as models transition from controlled research environments to real-world contexts, their reliability comes under heightened scrutiny. This thesis tackles the challenge of making ML systems more robust and trustworthy by placing the concept of \emph{uncertainty} at the forefront. 

% We first explore how \emph{selective prediction} mechanisms—where a model abstains on inputs deemed too uncertain—can markedly enhance reliability without modifying the core model architecture. By monitoring training dynamics and calibrating confidence estimates, we show that selective prediction can mitigate high-risk errors across tasks such as image classification, regression, and time-series analysis. We also investigate how requirements like \emph{differential privacy} introduce additional noise into the learning process, potentially undermining conventional uncertainty quantification methods, and we propose refinements to maintain reliability under privacy constraints.

% Next, we develop theoretical bounds that dissect the trade-off between the fraction of inputs on which a model provides predictions (coverage) and the accuracy on those accepted inputs. These results illuminate the role of calibration and confidence distributions in driving selective prediction performance. Beyond these methodological contributions, we examine adversarial scenarios wherein dishonest institutions manipulate uncertainty to deny services, highlighting the critical need for verifiability in abstention mechanisms. Finally, we illustrate how smaller local models and larger specialized models can be orchestrated to reduce computational overhead by deferring only complex queries to the more powerful model.

% Overall, this thesis provides a comprehensive framework—spanning empirical techniques, theoretical insights, and adversarial considerations—to design, evaluate, and deploy ML models capable of reliable decision-making under uncertainty. 

\noindent Machine learning (ML) systems are increasingly deployed in high-stakes domains where reliability is paramount. As these systems transition from research prototypes to real-world decision-makers, their ability to recognize and respond to uncertainty becomes essential. This thesis investigates how uncertainty estimation can enhance the safety and trustworthiness of ML, with a particular focus on selective prediction—a paradigm where models abstain from predicting when confidence is low.

We begin by showing that a model’s training trajectory contains rich signals that can be leveraged to estimate uncertainty without modifying the architecture or loss function. By ensembling predictions from intermediate checkpoints, we introduce a lightweight, post-hoc abstention mechanism that identifies unreliable predictions. This method applies across classification, regression, and time-series tasks, can be layered onto existing models, and avoids the training cost of deep ensembles while retaining much of their effectiveness. It achieves state-of-the-art performance on multiple selective prediction benchmarks and offers a practical solution for settings where retraining with specific uncertainty-enhancing loss functions is expensive or restricted.


The utility of this passive, post-hoc approach extends directly to another critical requirement for trustworthy AI: data privacy. %Since the high-stakes domains we target often involve sensitive personal data—from medical records to financial information—ensuring confidentiality is non-negotiable. 
Because our method merely observes the training trajectory, it remains fully compatible with formal privacy guarantees like differential privacy (DP). This unique compatibility allows us to investigate a crucial trade-off: how does the enforcement of privacy impact a model's ability to estimate its own uncertainty? We find that many standard methods degrade under DP noise, producing unreliable confidence scores. In contrast, our trajectory-based method remains robust. To fairly evaluate this trade-off, we propose a new framework that isolates the effect of privacy on uncertainty quality, enabling more meaningful comparisons between selective predictors in privacy-sensitive settings.

This motivates a theoretical study of what fundamentally limits selective prediction performance. We propose a finite-sample decomposition of the selective classification gap—the deviation from the oracle accuracy–coverage curve—and identify five key error sources: Bayes noise, approximation error, ranking error, statistical variability, and a residual term. This decomposition clarifies which levers—such as calibration, model capacity, or additional supervision—can close the gap, and explains why simple post-hoc calibration cannot address ranking imperfections, motivating methods that re-rank predictions based on more reliable uncertainty signals.

This analysis provides a blueprint for diagnosing and fixing the benign sources of error in a model. It assumes, however, that the model's uncertainty signals, while flawed, are an honest reflection of its internal state. This motivates a deeper investigation into scenarios where uncertainty signals are deliberately corrupted to mislead downstream decision-making. We show that the very mechanisms of ranking and calibration can be adversarially manipulated to suppress uncertainty in targeted regions or for specific user groups, enabling covert denial of service while maintaining high predictive performance. These attacks, which directly exploit the sources of error we identified, are hard to detect with standard evaluation. We therefore develop defenses that verify whether abstentions stem from genuine uncertainty, combining calibration audits with verifiable inference to ensure integrity. This highlights a broader lesson: trustworthy ML depends not just on estimating uncertainty well, but also on protecting it from manipulation.

Together, these contributions chart a path toward more reliable ML by studying how uncertainty can be estimated, evaluated, and safeguarded. The resulting systems not only make accurate predictions—but also know when to say “I do not know”.


