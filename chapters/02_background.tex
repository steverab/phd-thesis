\newcommand{\ba}[1]{\textbf{\sffamily #1}}

\newcommand{\sat}[0]{\ba{SAT}\xspace}
\newcommand{\satersr}[0]{\ba{SAT+ER+SR}\xspace}
\newcommand{\msp}[0]{\ba{MSP}\xspace}
\newcommand{\sr}[0]{\ba{SR}\xspace}
\newcommand{\sn}[0]{\ba{SN}\xspace}
\newcommand{\dg}[0]{\ba{DG}\xspace}
\newcommand{\odist}[0]{\ba{ODIST}\xspace}
\newcommand{\mcdo}[0]{\ba{MC-DO}\xspace}
\newcommand{\de}[0]{\ba{DE}\xspace}
\newcommand{\nntd}[0]{\ba{NNTD}\xspace}
\newcommand{\sctdde}[0]{\ba{DE+SCTD}\xspace}
\newcommand{\sctd}[0]{\ba{SCTD}\xspace}
\newcommand{\sptd}[0]{\ba{SPTD}\xspace}
\newcommand{\cclsc}[0]{\ba{CCL-SC}\xspace}
\newcommand{\aucoc}[0]{\ba{AUCOC}\xspace}
\newcommand{\temp}[0]{\ba{TEMP}\xspace}


\newcommand{\sptdde}[0]{\ba{DE+SPTD}\xspace}
\newcommand{\sptdc}[0]{\ba{SPTD-C}\xspace}
\newcommand{\sptdr}[0]{\ba{SPTD-R}\xspace}
\newcommand{\sptdts}[0]{\ba{SPTD-TS}\xspace}
\newcommand{\osp}[0]{\ba{OSP}\xspace}
\newcommand{\logitvar}[0]{\ba{LOGITVAR}\xspace}

\newcommand{\minscore}[0]{minimum score\xspace}
\newcommand{\avgscore}[0]{average score\xspace}
\newcommand{\jmpscore}[0]{jump score\xspace}
\newcommand{\varscore}[0]{variance score\xspace}
\newcommand{\smin}[0]{$s_\text{min}$\xspace}
\newcommand{\savg}[0]{$s_\text{avg}$\xspace}
\newcommand{\smax}[0]{$s_\text{MAX}$\xspace}
\newcommand{\ssum}[0]{$s_\text{SUM}$\xspace}
\newcommand{\swv}[0]{$s_\text{WV}$\xspace}
\newcommand{\swvr}[0]{$s_\text{WVR}$\xspace}
\newcommand{\swvts}[0]{$s_\text{WVTS}$\xspace}
\newcommand{\slast}[0]{$s_\text{last}$\xspace}
\newcommand{\sfull}[0]{$s_\text{full}$\xspace}
\newcommand{\sjmp}[0]{$s_\text{jmp}$\xspace}
\newcommand{\svar}[0]{$s_\text{var}$\xspace}
\newcommand{\fp}[0]{false-positive\xspace}
\newcommand{\fps}[0]{false-positives\xspace}
\newcommand{\ie}[0]{i.e.,\xspace}
\newcommand{\eg}[0]{e.g.,\xspace}
\newcommand{\selc}[0]{selective classification\xspace}
\newcommand{\selp}[0]{selective prediction\xspace}
\newcommand{\empiricalacccovtradeoff}[0]{$\text{acc}_{c}(f,g)$}
\newcommand{\upperbound}[0]{$\overline{\text{acc}}(a_\text{full},c)$}
\newcommand{\accnormscore}[0]{$s_{a_\text{full}}(f,g)$}
\newcommand{\realtradeoff}[0]{$\text{acc}_c(h,g)$}

\chapter{Background}
\label{ch:background}

\section{Introduction to Uncertainty Quantification}
Uncertainty quantification (UQ) is the process of identifying, quantifying, and managing the uncertainty inherent in computational and physical models. In machine learning and statistical modeling, UQ focuses on characterizing the confidence or trustworthiness of a model's predictions. Although machine learning models have grown very capable at making accurate predictions---especially with the advent of deep learning allowing for highly expressive models---understanding and communicating how certain (or uncertain) those predictions are remains a major challenge.

\subsection{Importance of Uncertainty Quantification}
Before diving into the specific reasons why uncertainty quantification is vital, it is helpful to note that UQ serves as a bridge between raw model output and actionable insight. While modern machine learning models can make highly accurate predictions, they do not inherently communicate their confidence level or the extent of variability within the data (i.e., Bayes error or irreducible error). This gap can be critical in real-world scenarios where misunderstandings or misapplications of model outputs can have significant consequences.

\paragraph{Decision-Making Under Risk.} In real-world applications like autonomous driving, healthcare, and finance, decisions often come with high stakes. Errors can lead to costly or even life-threatening outcomes, so having a clear sense of how certain a model’s predictions are is essential for risk management. For instance, in a clinical setting, an automated diagnostic tool that flags images as “uncertain” can trigger a secondary, human-led evaluation, thereby reducing the risk of misdiagnosis.
\paragraph{Model Validation and Trust.} A core challenge in deploying machine learning systems is understanding when and where they might fail. Uncertainty metrics, such as confidence intervals or Bayesian posterior distributions, help illuminate whether a model is systematically overconfident or underconfident in its predictions. Such insights guide model improvements (e.g., collecting more targeted training data) and bolster stakeholders’ trust by transparently indicating areas where the model may be less reliable.
\paragraph{Resource Allocation.} In settings with finite resources---for example, bandwidth, computational capacity, or expert availability---uncertainty estimates help allocate those resources more effectively. If a model can identify the instances where it is less certain, additional measures---such as further data collection, human review, or more computationally expensive algorithms---can be reserved for those high-uncertainty cases. This targeted approach prevents unnecessary overhead for predictions deemed sufficiently reliable.
\paragraph{Generalization and Reliability.} Overfitting, adversarial inputs, and domain shifts pose ongoing challenges in machine learning. When a model encounters data that differ from its training distribution, having robust uncertainty estimates can provide an early warning sign. For example, a model trained on data from one domain may exhibit higher uncertainty when faced with data from a new domain with differing characteristics. Such signals can prompt efforts to gather additional training data or adapt the model to the new distribution, ultimately improving its overall reliability and generalization.

\subsection{Approaches to Uncertainty Quantification}
There are several approaches to UQ in machine learning, reflecting different statistical and computational philosophies:

\paragraph{Bayesian Methods.}
In Bayesian inference, model parameters are treated as random variables~\citep{bishop2006pattern}. Given a set of observed data $\mathcal{D}$ and a parameter vector $\theta$, Bayes' theorem allows us to compute the posterior distribution:
\begin{equation}
p(\theta \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \theta) \, p(\theta)}{p(\mathcal{D})},
\end{equation}
where $p(\theta)$ is the prior, $p(\mathcal{D} \mid \theta)$ is the likelihood, and $p(\mathcal{D}) = \int p(\mathcal{D} \mid \theta)p(\theta)d\theta$ is the evidence. Techniques such as Markov Chain Monte Carlo (MCMC)~\citep{geyer1992practical} simulate draws from this posterior distribution, while Variational Inference (VI)~\citep{blei2017variational} seeks a tractable family of distributions $q(\theta)$ that approximates $p(\theta \mid \mathcal{D})$. Bayesian neural networks~\citep{blundell2015weight} extend these concepts to deep learning by placing priors over the network weights.

\paragraph{Frequentist and Distribution-Free Methods.}
Distribution-free approaches, like conformal prediction~\citep{shafer2008tutorial, fontana2023conformal}, aim to create prediction sets with finite-sample guarantees under assumptions such as exchangeability. For a new observation $\bm{x}$, conformal prediction constructs a set $\Gamma(\bm{x})$ such that
\begin{equation}
\Pr\{ Y \in \Gamma(\bm{X}) \} \;\geq\; 1 - \alpha,
\end{equation}
where $\alpha$ is a pre-specified significance level and $\bm{X}$ is the random variable. The construction typically involves calculating a nonconformity score for each example in a calibration set and then using these scores to determine the appropriate quantile for new predictions.

\paragraph{Ensemble Methods.}
Ensemble techniques~\citep{lakshminarayanan2017simple} combine the outputs of multiple independently trained models to capture uncertainty. Suppose we have an ensemble of $M$ models, $\{ f_1, f_2, \dots, f_M \}$. The aggregated prediction can be written as:
\begin{equation}
\hat{y} \;=\; \frac{1}{M} \sum_{m=1}^{M} f_m(\bm{x}),
\end{equation}
and the empirical variance, which serves as an uncertainty estimate, is given by:
\begin{equation}
\hat{\sigma}^2(\bm{x}) \;=\; \frac{1}{M-1} \sum_{m=1}^{M} \Bigl( f_m(\bm{x}) - \hat{y} \Bigr)^2.
\end{equation}
Although not inherently probabilistic, this approach provides a practical measure of prediction variability, particularly in high-dimensional settings.

\paragraph{Bootstrapping and Resampling.}
Bootstrapping involves generating multiple resampled datasets $\{\mathcal{D}^{(1)}, \mathcal{D}^{(2)}, \dots, \mathcal{D}^{(B)}\}$ from the original dataset $\mathcal{D}$ by sampling with replacement~\citep{breiman1996bagging}. For each bootstrap sample $\mathcal{D}^{(b)}$, a model $f^{(b)}$ is trained. The distribution of the model outputs at a given point $\bm{x}$ is then approximated as:
\begin{equation}
\hat{p}(y \mid \bm{x}) \;\approx\; \frac{1}{B} \sum_{b=1}^{B} \delta\bigl(y - f^{(b)}(\bm{x})\bigr),
\end{equation}
where $\delta$ denotes the Dirac delta function. This empirical distribution provides an estimate of the uncertainty in the model's predictions due to dataset variability.

\paragraph{Calibrated Models.}
Calibration techniques adjust a model’s predicted probabilities to better reflect true empirical frequencies. One common method is \emph{Platt scaling}~\citep{platt1999probabilistic}, which applies a sigmoid transformation to the model output:
\begin{equation}
p_{\text{cal}}(y \mid \bm{x}) \;=\; \frac{1}{1 + \exp\bigl(-a \, f(\bm{x}) - b\bigr)},
\end{equation}
where $a$ and $b$ are parameters learned on a validation set. A closely related method is \emph{temperature scaling}~\citep{guo2017calibration}, which calibrates the softmax outputs of a neural network by dividing the logits by a single scalar parameter $T > 0$, known as the temperature. Given the original logits $\bm{z}$, the calibrated probabilities are computed as:
\begin{equation}
p_{\text{cal}}(y \mid \bm{x}) \;=\; \frac{\exp(z_y / T)}{\sum_{k} \exp(z_k / T)}.
\end{equation}
Temperature scaling is a particularly simple yet effective post-hoc calibration method, as it preserves the model’s prediction ordering while improving the alignment between predicted confidence and true accuracy. Alternatively, \emph{isotonic regression}~\citep{zadrozny2002transforming} imposes a monotonicity constraint on the transformation function, allowing for a non-parametric calibration that adapts to the observed data distribution without assuming a specific functional form.




\section{Selective Prediction}
Selective prediction, sometimes called ``prediction with a reject option,'' or ``learning with abstention,'' introduces a mechanism for the model to \emph{abstain} (or ``reject'') on samples where its confidence is insufficiently high. In many high-stakes applications, having the option to reject can be valuable when the cost of a wrong decision exceeds the cost of not predicting at all.

In scenarios where an incorrect prediction could lead to severe consequences, it becomes essential to allow the model to defer judgment on uncertain cases. This selective approach not only helps to minimize potential risks by avoiding hasty decisions but also facilitates a more efficient allocation of resources by directing complex or ambiguous instances toward additional analysis or expert review. The following points further detail the driving factors behind adopting selective prediction in critical applications:

\begin{itemize}
    \item \textbf{Risk management}: In domains like medical diagnostics, autonomous driving, or finance, even a single misclassification can result in significant harm or financial loss. By rejecting predictions that do not meet a confidence threshold, the system minimizes the chance of error. This trade-off between coverage and accuracy ensures that only predictions with sufficiently low risk are acted upon, thereby safeguarding against the potentially high cost of a wrong decision.

    \item \textbf{Complex cost structures}: Many applications face asymmetric costs where the consequences of different types of errors vary dramatically. For instance, in healthcare, the cost of a false negative (failing to detect a disease) is often much higher than that of a false positive (unnecessary follow-up tests). A selective classifier can be tuned to consider these cost asymmetries by setting thresholds that balance the risk of errors against the operational costs of additional tests or interventions. This ensures that the system only makes predictions when the expected cost of a mistake is lower than the cost of deferring the decision.

    \item \textbf{Focus on ``easy'' cases}: In practice, models tend to perform well on typical examples while struggling with outliers or cases that lie near the decision boundary, especially under distribution shifts or when encountering rare events. By identifying and processing these ``easy'' cases automatically, the system can reserve more intensive, specialized methods (such as human review or more computationally demanding algorithms) for those uncertain or challenging instances. This tiered approach improves overall system efficiency and leverages expert resources only when they are truly needed.
\end{itemize}

% Consider a model \( f \) that provides a predictive output \(\hat{y}\) for an input \(x\). Along with the prediction, the model provides a confidence measure \( c(x) \). A \emph{selective classifier} introduces a \emph{selection function} \( g(x) \in \{0,1\} \). The model output is given by:
% \[
%   \tilde{y}(x) = 
%     \begin{cases} 
%       f(x), & \text{if } g(x) = 1, \\
%       \text{reject}, & \text{if } g(x) = 0.
%     \end{cases}
% \]

\subsection{Formal Definition} 

Selective prediction extends the standard supervised classification framework by allowing the model to output a special \emph{rejection} symbol~$\bot$ through the use of a \textit{gating function}~\citep{yaniv2010riskcoveragecurve}. This gating function consults the underlying classifier and returns a prediction only when it is sufficiently confident in its correctness; otherwise, it opts to abstain. Typically, the gating decision is derived directly from the behavior of the classifier $f$, and we make this dependency explicit in our formulation. Specifically, we define a selection function $g: \mathcal{X} \times (\mathcal{X} \rightarrow \mathcal{Y}) \rightarrow \mathbb{R}$, which evaluates whether the model should produce a prediction for a given input~$\bm{x}$. If the value of $g(\bm{x}, f)$ is less than or equal to a predefined threshold $\tau$, the classifier proceeds with the prediction $f(\bm{x})$; otherwise, it abstains by returning $\bot$. This defines the joint selective prediction model as:
\begin{equation}
    (f,g)(\bm{x}) = \begin{cases}
  f(\bm{x})  & g(\bm{x}, f) \leq \tau \\
  \bot & \text{otherwise.}
\end{cases}
\end{equation}

\subsection{Key Theoretical Results}
Seminal works such as \citet{chow1957optimum} and \citet{el2010foundations} established theoretical foundations for selective prediction and derived performance bounds. Below, we provide more formal statements of three key results.

\paragraph{Chow's Rule.}
In a classification problem with a zero-one loss plus a reject penalty $c_r > 0$, let $p(y \mid \bm{x})$ denote the posterior probability of class $y$ given input $\bm{x}$. Then Chow's rule \citep{chow1957optimum} specifies that the optimal policy is to reject whenever the highest posterior probability falls below a threshold determined by $c_r$. More precisely, define
\begin{equation}
\hat{y}(\bm{x}) \;=\; \arg\max_{y\in\mathcal{Y}} \;p(y \mid \bm{x}),
\end{equation}
and let 
\begin{equation}
p_{\max}(\bm{x}) \;=\; \max_{y\in\mathcal{Y}} \; p(y \mid \bm{x}).
\end{equation}
\citet{chow1957optimum} showed that the \emph{Bayes optimal} decision rule in this setting is:
\begin{equation}
(f^\ast, g^\ast)(\bm{x}) \;=\; \begin{cases}
\hat{y}(\bm{x}) \quad &\text{if } p_{\max}(\bm{x}) \;\geq\; \theta(c_r) \\
\bot \quad &\text{otherwise},
\end{cases}
\end{equation}
where the threshold $\theta(c_r)$ depends on the reject cost $c_r$ and the class prior probabilities. By rejecting low-confidence instances, this rule minimizes the expected combined cost of misclassifications and rejections.

\paragraph{Coverage Guarantees.}
In a \emph{distribution-free} setting, conformal prediction provides a systematic way to construct prediction sets $\Gamma(\bm{x})$ with finite-sample coverage guarantees, assuming i.i.d. data and certain mild conditions (e.g., exchangeability). Formally, for a given miscoverage rate $\alpha \in (0,1)$, conformal prediction ensures:
\begin{equation}
\Pr_{\bm{x}, y}\bigl\{ y \notin \Gamma(\bm{x}) \bigr\} \;\le\; \alpha
\end{equation}
with high probability over the random draw of a calibration set. One can interpret a ``reject'' as returning either an empty set $\Gamma(\bm{x}) = \varnothing$ or a set so large that it does not provide a useful prediction. This approach generalizes to both classification and regression contexts, offering a principled way to balance coverage against set size (and hence confidence).

\paragraph{PAC-Style Bounds.}
Within the Probably Approximately Correct (PAC) framework, one can analyze selective classification by bounding the probability that the error on the covered set exceeds a given threshold. Let $R(f,g)$ be the expected error of a selective classifier $(f,g)$ on the subset of instances it does not reject. Under typical PAC assumptions (i.i.d. samples and bounded loss), there exist results showing that, with high probability over the training set of size $n$,
\begin{equation}
R(f,g) \;\le\; \epsilon \quad \text{and} \quad \text{coverage}(f,g) \;\geq\; 1-\alpha
\end{equation}
for sufficiently large $n$~\citep{cortes2016learning}. Here, $\epsilon$ and $\alpha$ quantify tolerances on error and coverage, respectively. In essence, these theorems illustrate that by allowing a controlled reject rate, one can significantly decrease the classification error on the accepted samples, provided enough training data are available to reliably estimate the gating function.

\subsection{Common Selective Prediction Techniques}

\paragraph{Softmax Response (\sr).} The traditional baseline methods for selective prediction is the \emph{Softmax Response} (\sr) method \citep{hendrycks2016baseline, geifman2017selective}. This method uses the confidence of a prediction model $f$ as the selection score:
\begin{equation}
	g_{\sr}(\bm{x}, f) = \max_{c \in C} f(\bm{x})
\end{equation}
Note that we assume here that $f$ produces a categorical distribution over $C$ classes. While this method is easy to implement and does not incur any additional cost, \sr has been found to be overconfident on ambiguous, hard-to-classify, or unrecognizable inputs.

\paragraph{SelectiveNet (\sn).} 
A variety of selective classification methods have been proposed that leverage explicit architecture and loss function adaptations. For example, \emph{SelectiveNet} (\sn) \citep{geifman2019selectivenet} modifies the model architecture to jointly optimize $(f,g)$ while targeting the model at a desired coverage level~$c_\text{target}$. The augmented model consists of a representation function $r: \mathcal{X} \rightarrow \mathbb{R}^L$ mapping inputs to latent codes and three additional functions: (i) the \emph{prediction} function $f: \mathbb{R}^L \rightarrow \mathbb{R}^C$ for the classification task targeted at~$c_\text{target}$; (ii) the \emph{selection} function $g: \mathbb{R}^L \rightarrow [0,1]$ representing a continuous approximation of the accept/reject decision for $\bm{x}$; and (iii) an additional \emph{auxiliary} function $h: \mathbb{R}^L \rightarrow \mathbb{R}^C$ trained for the unconstrained classification tasks. This yields the following losses:
 \begin{align}
 	\mathcal{L} & = \alpha \mathcal{L}_{f,g} + (1-\alpha) \mathcal{L}_h \\
 	\mathcal{L}_{f,g} & = \frac{\frac{1}{M}\sum_{m=1}^{M} \ell( f \circ r(\bm{x}_m) , y_m)}{\text{cov}(f,g)} + \lambda \max(0, c - \text{cov}(f,g))^2 \\
 	\mathcal{L}_{h} & = \frac{1}{M}\sum_{m=1}^{M} \ell( h \circ r(\bm{x}_m) , y_m)
 \end{align}
 The selection score for a particular point $\bm{x}$ is then given by:
 \begin{equation}
 	g_{\sn}(\bm{x}, f) = \sigma(g \circ r(\bm{x})) = \frac{1}{1 + \exp(g \circ r(\bm{x}))}
 \end{equation}

 \paragraph{Self-Adaptive Training (\sat).}
 Alternatively, prior works like Deep Gamblers~\citep{liu2019deep} and \emph{Self-Adaptive Training}~\citep{huang2020self} have also considered explicitly modeling the abstention class $\bot$ and adapting the optimization process to provide a learning signal for this class. For instance, \emph{Self-Adaptive Training} (\sat) incorporates information obtained during the training process into the optimization itself by computing and monitoring an exponential moving average of training point predictions over the training process. Samples with high prediction uncertainty are then used for training the abstention class. To ensure that the exponential moving average captures the true prediction uncertainty, an initial burn-in phase is added to the training procedure. This delay allows the model to first optimize the non-augmented, \ie original $C$-class prediction task and optimize for selective classification during the remainder of the training process. The updated loss is defined as:
 \begin{equation}
 	\mathcal{L} = -\frac{1}{M}\sum_{m=1}^{M} \left ( t_{i,y_i}\log p_{i,y_i} + (1-t_{i,y_i})\log p_{i,C+1} \right )
 \end{equation}
The abstention decision is then determined by the degree of confidence in the rejection class:
\begin{equation}
	g_{\sat}(\bm{x}, f) = f(\bm{x})_{C+1}
\end{equation}

 \paragraph{Deep Ensembles (\de).}
 Finally, ensemble methods combine the information content of $M$ models into a single final model. Since these models approximate the variance of the underlying prediction problem, they are often used for the purpose of uncertainty quantification and, by extension, \selp. The canonical instance of this approach for deep learning based models, \emph{Deep Ensembles}~(\de)~\citep{lakshminarayanan2017simple}, trains multiple models from scratch with varying initializations using a proper scoring rule and adversarial training. Then, after averaging the predictions made by the model, the softmax response (\sr) mechanism is applied:
 \begin{equation}
 	g_{\de}(\bm{x}, f) = \max_{c \in C} \frac{1}{M} \sum_{m=1}^{M} f_{\bm{\theta}_{m,T}}(\bm{x}).
 \end{equation}
 
 \paragraph{Monte-Carlo Dropout (\mcdo).} 
  To overcome the computational cost of estimating multiple models from scratch, \emph{Monte-Carlo Dropout} (\mcdo) \citep{gal2016dropout} allows for bootstrapping of model uncertainty of a dropout-equipped model at test time. While dropout is predominantly used during training to enable regularization of deep neural nets, it can also be used at test time to yield a random sub-network of the full neural network. Concretely, given a model $f$ with dropout-probability $o$, we can generate $M$ random sub-networks at test-time by deactivating a fraction $o$ of nodes. For a given test input $\bm{x}$ we can then average the outputs over all models and apply softmax response (\sr):
 \begin{equation}
 	g_{\mcdo}(\bm{x}, f) = \max_{c \in C} \frac{1}{Q} \sum_{q=1}^{Q} f_{o(\bm{\theta}_{T})}(\bm{x})
 \end{equation}

% \subsection{Practical Implementations}
% \begin{itemize}
%     \item \textbf{Threshold-based selective classifiers}:
%     Compute a confidence score for each prediction (for classification, often the predicted probability of the chosen class). Reject if the score is below a threshold. Neural network classifiers often use the maximum softmax probability as a (crude) confidence estimate.

%     \item \textbf{Bayesian or ensemble-based methods}:
%     Obtain a posterior or ensemble predictive distribution (e.g., through a Bayesian neural network or multiple models). Calculate a measure of uncertainty like predictive entropy or variance and reject if it exceeds a certain threshold.

%     \item \textbf{Conformal predictors with region prediction}:
%     Conformal methods construct prediction sets that satisfy a desired coverage level. A small or empty set indicates high uncertainty, which can be interpreted as a reject decision.

%     \item \textbf{Cost-sensitive frameworks}:
%     Incorporate a specific cost for rejection alongside the misclassification cost, and learn thresholds that minimize overall risk.
% \end{itemize}




\subsection{Challenges and Trade-offs}
In selective prediction, a model’s performance depends not only on its accuracy when it decides to predict but also on how frequently it abstains. This leads to a fundamental \emph{coverage-utility trade-off}: increasing coverage (i.e., predicting on more samples) can degrade average performance, whereas being too selective may ignore a large portion of the data. In addition, various practical factors such as calibration, cost specification, and the nature of the output space further complicate the deployment of selective classifiers in real-world applications.

\paragraph{Calibration vs.\ Thresholding.}
A common approach to selective prediction is to set a confidence threshold below which the model abstains. However, if the model’s confidence scores are poorly calibrated, even a well-chosen threshold may yield suboptimal coverage and performance. Calibration techniques (e.g., Platt scaling or isotonic regression) are often necessary to align predicted confidence with actual likelihoods of correctness. Striking the right balance between calibration and thresholding is crucial for optimizing both utility (accuracy on covered samples) and coverage.

\paragraph{Data Distribution Shifts.}
In practice, data encountered during deployment may differ substantially from the training distribution. Under such domain shifts, confidence estimates can become unreliable, leading the gating mechanism to either abstain too frequently or incorrectly cover samples outside the training distribution. Techniques that adapt or re-estimate confidence under shifting conditions—such as online calibration or domain adaptation—remain an active area of research, aiming to preserve the coverage-utility trade-off even in evolving environments.

\paragraph{Cost Specification.}
Selective prediction often relies on a clear notion of the relative costs of rejection and misclassification. However, specifying these costs can be non-trivial, as it depends on domain knowledge, user preferences, and application-specific constraints. For instance, in a medical setting, the cost of a missed diagnosis may far outweigh the inconvenience of additional tests. An inaccurate cost specification can skew the thresholding decisions, leading to suboptimal coverage and utility outcomes.

\paragraph{Complex Outputs.}
While selective classification is well-understood for simple categorical outputs, tasks such as semantic segmentation or other structured prediction problems pose additional difficulties. Rejecting an entire structured output may be too coarse, yet partial rejections introduce substantial complexity into model design and training protocols. Extending selective prediction methods to these richer output spaces requires careful definition of what abstaining means (e.g., rejecting certain regions of an image segmentation vs.\ rejecting the entire image) and how to maintain favorable coverage-utility characteristics.


\subsection{Relationship Between UQ and Selective Prediction}

Whereas uncertainty quantification (UQ) can exist independently—for instance, by reporting confidence intervals or predictive distributions—selective prediction leverages these uncertainty estimates to decide whether to abstain from making a prediction. When reliable confidence or uncertainty metrics are available, selective classification naturally follows as a strategy for reducing risk. In particular, tools from uncertainty quantification—such as predictive distributions, confidence intervals, or calibration measures—yield scores or metrics (e.g., variance, entropy, or calibration error) that help determine whether the model should provide a label or abstain. However, for these signals to be effective, they must be both \emph{accurate} and \emph{well-calibrated}: miscalibrated or otherwise inaccurate estimates of uncertainty can lead to poor coverage-accuracy trade-offs, ultimately reducing the model’s reliability.

In this sense, selective prediction is an \textbf{action} grounded in the \textbf{estimation} process provided by uncertainty quantification. The decision to abstain hinges critically on the fidelity of the underlying uncertainty estimates. Without well-calibrated uncertainty, even sophisticated abstention mechanisms can perform poorly, either abstaining unnecessarily or failing to abstain when the risk is high.

More broadly, uncertainty estimation acquires practical significance only when situated within a downstream decision-making framework. Uncertainty—such as a predictive variance or entropy value—is not inherently meaningful unless it guides a choice among competing actions. In many real-world scenarios, decision-making under uncertainty is the norm: a medical diagnosis may trigger treatment or further testing, a financial risk estimate may inform loan approval, and a self-driving car’s confidence in its environment perception may determine whether it continues autonomously or defers control. In such contexts, the value of uncertainty lies in how it informs risk-aware decision-making.

Selective prediction provides a concrete instantiation of this principle. It formalizes a decision problem in which the model must choose between predicting and abstaining for each input, effectively balancing coverage against accuracy. Here, uncertainty is not just a descriptive statistic—it becomes a functional component of the model’s behavior. By embedding uncertainty into the decision rule itself, selective prediction highlights how the utility of uncertainty is deeply intertwined with the structure of the decision problem. Uncertainty matters most when there is something at stake and when there are viable alternatives—such as deferring to a human or a more accurate model—that can mitigate the consequences of model error.



% \section*{References (Selected)}
% \begin{itemize}
%     \item Chow, C.~K. (1957). \emph{An optimum character recognition system using decision functions}. IRE Transactions on Electronic Computers, 6, 247--254.
%     \item El-Yaniv, R. and Wiener, Y. (2010). \emph{On the foundations of noise-free selective classification}. Journal of Machine Learning Research, 11, 1605--1641.
%     \item Vovk, V., Gammerman, A., \& Shafer, G. (2005). \emph{Algorithmic Learning in a Random World.} Springer.
%     \item Geifman, Y. \& El-Yaniv, R. (2017). \emph{Selective classification for deep neural networks}. NIPS Workshop on Deep Learning: Bridging Theory and Practice.
%     \item Gal, Y. \& Ghahramani, Z. (2016). \emph{Dropout as a Bayesian approximation: Representing model uncertainty in deep learning}. Proceedings of the 33rd International Conference on Machine Learning.
%     \item Lakshminarayanan, B., Pritzel, A., \& Blundell, C. (2017). \emph{Simple and scalable predictive uncertainty estimation using deep ensembles}. Advances in Neural Information Processing Systems.
% \end{itemize}
