\newcommand{\ba}[1]{\textbf{\sffamily #1}}

\newcommand{\sat}[0]{\ba{SAT}\xspace}
\newcommand{\satersr}[0]{\ba{SAT+ER+SR}\xspace}
\newcommand{\msp}[0]{\ba{MSP}\xspace}
\newcommand{\sr}[0]{\ba{SR}\xspace}
\newcommand{\sn}[0]{\ba{SN}\xspace}
\newcommand{\dg}[0]{\ba{DG}\xspace}
\newcommand{\odist}[0]{\ba{ODIST}\xspace}
\newcommand{\mcdo}[0]{\ba{MC-DO}\xspace}
\newcommand{\de}[0]{\ba{DE}\xspace}
\newcommand{\nntd}[0]{\ba{NNTD}\xspace}
\newcommand{\sctdde}[0]{\ba{DE+SCTD}\xspace}
\newcommand{\sctd}[0]{\ba{SCTD}\xspace}
\newcommand{\sptd}[0]{\ba{SPTD}\xspace}
\newcommand{\cclsc}[0]{\ba{CCL-SC}\xspace}
\newcommand{\aucoc}[0]{\ba{AUCOC}\xspace}
\newcommand{\temp}[0]{\ba{TEMP}\xspace}


\newcommand{\sptdde}[0]{\ba{DE+SPTD}\xspace}
\newcommand{\sptdc}[0]{\ba{SPTD-C}\xspace}
\newcommand{\sptdr}[0]{\ba{SPTD-R}\xspace}
\newcommand{\sptdts}[0]{\ba{SPTD-TS}\xspace}
\newcommand{\osp}[0]{\ba{OSP}\xspace}
\newcommand{\logitvar}[0]{\ba{LOGITVAR}\xspace}

\newcommand{\minscore}[0]{minimum score\xspace}
\newcommand{\avgscore}[0]{average score\xspace}
\newcommand{\jmpscore}[0]{jump score\xspace}
\newcommand{\varscore}[0]{variance score\xspace}
\newcommand{\smin}[0]{$s_\text{min}$\xspace}
\newcommand{\savg}[0]{$s_\text{avg}$\xspace}
\newcommand{\smax}[0]{$s_\text{MAX}$\xspace}
\newcommand{\ssum}[0]{$s_\text{SUM}$\xspace}
\newcommand{\swv}[0]{$s_\text{WV}$\xspace}
\newcommand{\swvr}[0]{$s_\text{WVR}$\xspace}
\newcommand{\swvts}[0]{$s_\text{WVTS}$\xspace}
\newcommand{\slast}[0]{$s_\text{last}$\xspace}
\newcommand{\sfull}[0]{$s_\text{full}$\xspace}
\newcommand{\sjmp}[0]{$s_\text{jmp}$\xspace}
\newcommand{\svar}[0]{$s_\text{var}$\xspace}
\newcommand{\fp}[0]{false-positive\xspace}
\newcommand{\fps}[0]{false-positives\xspace}
\newcommand{\ie}[0]{i.e.,\xspace}
\newcommand{\eg}[0]{e.g.,\xspace}
\newcommand{\selc}[0]{selective classification\xspace}
\newcommand{\selp}[0]{selective prediction\xspace}
\newcommand{\empiricalacccovtradeoff}[0]{$\text{acc}_{c}(f,g)$}
\newcommand{\upperbound}[0]{$\overline{\text{acc}}(a_\text{full},c)$}
\newcommand{\accnormscore}[0]{$s_{a_\text{full}}(f,g)$}
\newcommand{\realtradeoff}[0]{$\text{acc}_c(h,g)$}

\chapter{Background}
\label{ch:background}

\section{Introduction to Uncertainty Quantification}
Uncertainty quantification (UQ) is the process of identifying, quantifying, and managing the uncertainty inherent in computational and physical models. In machine learning and statistical modeling, UQ focuses on characterizing the confidence or trustworthiness of a model's predictions. Although machine learning models have grown very capable at making accurate predictions---especially with the advent of deep learning allowing for highly expressive models---understanding and communicating how certain (or uncertain) those predictions are remains a major challenge.

\subsection{Importance of Uncertainty Quantification}
Before diving into the specific reasons why uncertainty quantification is vital, it is helpful to note that UQ serves as a bridge between raw model output and actionable insight. While modern machine learning models can make highly accurate predictions, they do not inherently communicate their confidence level or the extent of variability within the data (i.e., Bayes error or irreducible error). This gap can be critical in real-world scenarios where misunderstandings or misapplications of model outputs can have significant consequences.

\paragraph{Decision-Making Under Risk.} In real-world applications like autonomous driving, healthcare, and finance, decisions often come with high stakes. Errors can lead to costly or even life-threatening outcomes, so having a clear sense of how certain a model’s predictions are is essential for risk management. For instance, in a clinical setting, an automated diagnostic tool that flags images as “uncertain” can trigger a secondary, human-led evaluation, thereby reducing the risk of misdiagnosis.
\paragraph{Model Validation and Trust.} A core challenge in deploying machine learning systems is understanding when and where they might fail. Uncertainty metrics, such as confidence intervals or Bayesian posterior distributions, help illuminate whether a model is systematically overconfident or underconfident in its predictions. Such insights guide model improvements (e.g., collecting more targeted training data) and bolster stakeholders’ trust by transparently indicating areas where the model may be less reliable.
\paragraph{Resource Allocation.} In settings with finite resources---for example, bandwidth, computational capacity, or expert availability---uncertainty estimates help allocate those resources more effectively. If a model can identify the instances where it is less certain, additional measures---such as further data collection, human review, or more computationally expensive algorithms---can be reserved for those high-uncertainty cases. This targeted approach prevents unnecessary overhead for predictions deemed sufficiently reliable.
\paragraph{Generalization and Reliability.} Overfitting, adversarial inputs, and domain shifts pose ongoing challenges in machine learning. When a model encounters data that differ from its training distribution, having robust uncertainty estimates can provide an early warning sign. For example, a model trained on data from one domain may exhibit higher uncertainty when faced with data from a new domain with differing characteristics. Such signals can prompt efforts to gather additional training data or adapt the model to the new distribution, ultimately improving its overall reliability and generalization.

\subsection{Approaches to Uncertainty Quantification}
There are several approaches to UQ in machine learning, reflecting different statistical and computational philosophies:

\paragraph{Bayesian Methods.}
In Bayesian inference, model parameters are treated as random variables~\citep{bishop2006pattern}. Given a set of observed data $\mathcal{D}$ and a parameter vector $\theta$, Bayes' theorem allows us to compute the posterior distribution:
\begin{equation}
p(\theta \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \theta) \, p(\theta)}{p(\mathcal{D})},
\end{equation}
where $p(\theta)$ is the prior, $p(\mathcal{D} \mid \theta)$ is the likelihood, and $p(\mathcal{D}) = \int p(\mathcal{D} \mid \theta)p(\theta)d\theta$ is the evidence. Techniques such as Markov Chain Monte Carlo (MCMC)~\citep{geyer1992practical} simulate draws from this posterior distribution, while Variational Inference (VI)~\citep{blei2017variational} seeks a tractable family of distributions $q(\theta)$ that approximates $p(\theta \mid \mathcal{D})$. Bayesian neural networks~\citep{blundell2015weight} extend these concepts to deep learning by placing priors over the network weights.

\paragraph{Frequentist and Distribution-Free Methods.}
Distribution-free approaches, like conformal prediction~\citep{shafer2008tutorial, fontana2023conformal}, aim to create prediction sets with finite-sample guarantees under assumptions such as exchangeability. For a new observation $\bm{x}$, conformal prediction constructs a set $\Gamma(\bm{x})$ such that
\begin{equation}
\Pr\{ Y \in \Gamma(\bm{X}) \} \;\geq\; 1 - \alpha,
\end{equation}
where $\alpha$ is a pre-specified significance level and $\bm{X}$ is the random variable. The construction typically involves calculating a nonconformity score for each example in a calibration set and then using these scores to determine the appropriate quantile for new predictions.

\paragraph{Ensemble Methods.}
Ensemble techniques~\citep{lakshminarayanan2017simple} combine the outputs of multiple independently trained models to capture uncertainty. Suppose we have an ensemble of $M$ models, $\{ f_1, f_2, \dots, f_M \}$. The aggregated prediction can be written as:
\begin{equation}
\hat{y} \;=\; \frac{1}{M} \sum_{m=1}^{M} f_m(\bm{x}),
\end{equation}
and the empirical variance, which serves as an uncertainty estimate, is given by:
\begin{equation}
\hat{\sigma}^2(\bm{x}) \;=\; \frac{1}{M-1} \sum_{m=1}^{M} \Bigl( f_m(\bm{x}) - \hat{y} \Bigr)^2.
\end{equation}
Although not inherently probabilistic, this approach provides a practical measure of prediction variability, particularly in high-dimensional settings.

\paragraph{Bootstrapping and Resampling.}
Bootstrapping involves generating multiple resampled datasets $\{\mathcal{D}^{(1)}, \mathcal{D}^{(2)}, \dots, \mathcal{D}^{(B)}\}$ from the original dataset $\mathcal{D}$ by sampling with replacement~\citep{breiman1996bagging}. For each bootstrap sample $\mathcal{D}^{(b)}$, a model $f^{(b)}$ is trained. The distribution of the model outputs at a given point $\bm{x}$ is then approximated as:
\begin{equation}
\hat{p}(y \mid \bm{x}) \;\approx\; \frac{1}{B} \sum_{b=1}^{B} \delta\bigl(y - f^{(b)}(\bm{x})\bigr),
\end{equation}
where $\delta$ denotes the Dirac delta function. This empirical distribution provides an estimate of the uncertainty in the model's predictions due to dataset variability.

\paragraph{Calibrated Models.}
Calibration techniques adjust a model’s predicted probabilities to better reflect true empirical frequencies. One common method is \emph{Platt scaling}~\citep{platt1999probabilistic}, which applies a sigmoid transformation to the model output:
\begin{equation}
p_{\text{cal}}(y \mid \bm{x}) \;=\; \frac{1}{1 + \exp\bigl(-a \, f(\bm{x}) - b\bigr)},
\end{equation}
where $a$ and $b$ are parameters learned on a validation set. A closely related method is \emph{temperature scaling}~\citep{guo2017calibration}, which calibrates the softmax outputs of a neural network by dividing the logits by a single scalar parameter $T > 0$, known as the temperature. Given the original logits $\bm{z}$, the calibrated probabilities are computed as:
\begin{equation}
p_{\text{cal}}(y \mid \bm{x}) \;=\; \frac{\exp(z_y / T)}{\sum_{k} \exp(z_k / T)}.
\end{equation}
Temperature scaling is a particularly simple yet effective post-hoc calibration method, as it preserves the model’s prediction ordering while improving the alignment between predicted confidence and true accuracy. Alternatively, \emph{isotonic regression}~\citep{zadrozny2002transforming} imposes a monotonicity constraint on the transformation function, allowing for a non-parametric calibration that adapts to the observed data distribution without assuming a specific functional form.

\section{Selective Prediction}
Selective prediction, sometimes called ``prediction with a reject option,'' or ``learning with abstention,'' introduces a mechanism for the model to \emph{abstain} (or ``reject'') on samples where its confidence is insufficiently high. In many high-stakes applications, having the option to reject can be valuable when the cost of a wrong decision exceeds the cost of not predicting at all.

In scenarios where an incorrect prediction could lead to severe consequences, it becomes essential to allow the model to defer judgment on uncertain cases. This selective approach not only helps to minimize potential risks by avoiding hasty decisions but also facilitates a more efficient allocation of resources by directing complex or ambiguous instances toward additional analysis or expert review. The following points further detail the driving factors behind adopting selective prediction in critical applications:

\begin{itemize}
    \item \textbf{Risk management}: In domains like medical diagnostics, autonomous driving, or finance, even a single misclassification can result in significant harm or financial loss. By rejecting predictions that do not meet a confidence threshold, the system minimizes the chance of error. This trade-off between coverage and accuracy ensures that only predictions with sufficiently low risk are acted upon, thereby safeguarding against the potentially high cost of a wrong decision.

    \item \textbf{Complex cost structures}: Many applications face asymmetric costs where the consequences of different types of errors vary dramatically. For instance, in healthcare, the cost of a false negative (failing to detect a disease) is often much higher than that of a false positive (unnecessary follow-up tests). A selective classifier can be tuned to consider these cost asymmetries by setting thresholds that balance the risk of errors against the operational costs of additional tests or interventions. This ensures that the system only makes predictions when the expected cost of a mistake is lower than the cost of deferring the decision.

    \item \textbf{Focus on ``easy'' cases}: In practice, models tend to perform well on typical examples while struggling with outliers or cases that lie near the decision boundary, especially under distribution shifts or when encountering rare events. By identifying and processing these ``easy'' cases automatically, the system can reserve more intensive, specialized methods (such as human review or more computationally demanding algorithms) for those uncertain or challenging instances. This tiered approach improves overall system efficiency and leverages expert resources only when they are truly needed.
\end{itemize}

\subsection{Formal Definition} 

Selective prediction extends the standard supervised classification framework by allowing the model to output a special \emph{rejection} symbol~$\bot$ through the use of a \textit{gating function}~\citep{yaniv2010riskcoveragecurve}. This gating function consults the underlying classifier and returns a prediction only when it is sufficiently confident in its correctness; otherwise, it opts to abstain. 

Throughout this thesis, we assume that inputs belong to a covariate space~$\mathcal{X} \subseteq \mathbb{R}^d$. The classifier $f$ maps each input either to a hard label in the set $\mathcal{Y} = [C] = \{1,\dots,C\}$, \emph{or} to a softmax probability vector in the simplex $\mathcal{Y} = \Delta^{C-1} \subset [0,1]^C$. The latter convention covers cases where the gating decision uses scores such as maximum class probability, entropy, or logit margins.

Typically, the gating decision is derived directly from the behavior of the classifier $f$, and we make this dependency explicit in our formulation. Specifically, we define a selection function $g: \mathcal{X} \times (\mathcal{X} \rightarrow \mathcal{Y}) \rightarrow \mathbb{R}$, which evaluates whether the model should produce a prediction for a given input~$\bm{x}$. If the value of $g(\bm{x}, f)$ is less than or equal to a predefined threshold $\tau$, the classifier proceeds with the prediction $f(\bm{x})$; otherwise, it abstains by returning $\bot$. This defines the joint selective prediction model as:
\begin{equation}
    (f,g)(\bm{x}) = \begin{cases}
      f(\bm{x}) & g(\bm{x}, f) \leq \tau \\
      \bot & \text{otherwise.}
    \end{cases}
\end{equation}
For clarity, we may occasionally drop the explicit dependence of $g$ on $f$ and simply write $g(\bm{x})$ when the context makes it unambiguous.

\subsection{Key Theoretical Results}
Seminal works such as \citet{chow1957optimum} and \citet{el2010foundations} established theoretical foundations for selective prediction and derived performance bounds. Below, we provide more formal statements of three key results.

\paragraph{Chow's Rule.}
In selective classification, Chow’s rule adopts a simplified cost-sensitive objective that assigns a fixed penalty for abstaining from a prediction. Specifically, the cost model assigns:
\begin{itemize}
  \item zero cost for correct predictions,
  \item unit cost for incorrect predictions (i.e., standard 0–1 loss), and
  \item a fixed reject cost $c_r > 0$ for abstaining.
\end{itemize}
Formally, for a decision function $(f,g)$, the expected risk under this cost model is
\begin{equation}
R(f,g) \;=\; \mathbb{E} \big[ \mathbb{I}\{g(\bm{x}) = 1, f(\bm{x}) \neq y\} \;+\; c_r \,\mathbb{I}\{g(\bm{x}) = 0\} \big],
\end{equation}
where $g(\bm{x}) \in \{0,1\}$ indicates whether the classifier predicts ($1$) or abstains ($0$).  
Let $p(y \mid \bm{x})$ denote the posterior probability of class $y$ given input $\bm{x}$. Then Chow's rule \citep{chow1957optimum} specifies that the decision that minimizes this expected cost—i.e., the \emph{Bayes optimal} decision under this cost model—is to predict the most likely label if the confidence (posterior probability) exceeds a threshold, and to reject otherwise. Formally, define
\begin{equation}
\hat{y}(\bm{x}) \;=\; \arg\max_{y\in\mathcal{Y}} \;p(y \mid \bm{x}),
\end{equation}
and let 
\begin{equation}
p_{\max}(\bm{x}) \;=\; \max_{y\in\mathcal{Y}} \; p(y \mid \bm{x}).
\end{equation}
Then the Bayes optimal selective classifier is:
\begin{equation}
(f^\ast, g^\ast)(\bm{x}) \;=\; \begin{cases}
\hat{y}(\bm{x}) \quad &\text{if } p_{\max}(\bm{x}) \;\geq\; \theta(c_r) \\
\bot \quad &\text{otherwise},
\end{cases}
\end{equation}
where the threshold $\theta(c_r)$ depends on both the reject cost $c_r$ and the class priors. This threshold balances the expected cost of an incorrect prediction against the fixed cost of abstaining, thereby minimizing the total expected cost across the distribution of inputs.


\paragraph{Coverage Guarantees and Set Utility.}
In a \emph{distribution-free} setting, conformal prediction~\citep{angelopoulos2021gentle} offers a principled way to build prediction sets $\Gamma(\bm{x})$ that satisfy finite-sample coverage guarantees under i.i.d.\ (exchangeable) data. Specifically, for any target miscoverage rate $\alpha\in(0,1)$,
\begin{equation}
  \Pr_{\bm{x},y}\!\bigl\{\,y\notin\Gamma(\bm{x})\,\bigr\}\;\le\;\alpha,
\end{equation}
with high probability over the random draw of the calibration set.  

While this bound limits how often the true label is excluded, it says nothing about \emph{how informative} the returned set is when coverage holds.  A widely accepted proxy for informativeness is the set’s \emph{size}: the cardinality $\lvert\Gamma(\bm{x})\rvert$ in classification or the length/volume in regression.  Size is appealing for three complementary reasons. 
\begin{enumerate}
    \item \textbf{Decision-theoretic grounding:} in many downstream tasks a user must act on \emph{any} element of $\Gamma(\bm{x})$ (e.g., prescribe the safest drug, choose the cheapest feasible route).  Under a worst-case or cost-per-option assumption, the expected utility of a set is monotone in its size, so minimizing size is equivalent to minimizing expected cost. 
    \item \textbf{Statistical tractability:} size admits clean optimality results—e.g.\ under the “marginal coverage with minimal expected size’’ criterion, conformal p-value ordering is provably optimal among all measurable set-valued predictors with the same coverage level.
    \item \textbf{Operational simplicity:} size provides an intuitive, threshold-friendly signal.  Practitioners can stipulate an application-specific upper bound $\tau$ on acceptable set size; if $\lvert\Gamma(\bm{x})\rvert>\tau$, the set is deemed uninformative and the system instead “defers’’ or “rejects.’’  Consequently, abstention can occur either explicitly ($\Gamma(\bm{x})=\varnothing$) or implicitly (oversized $\Gamma(\bm{x})$), unifying conformal prediction with selective classification via a simple size-based rule.
\end{enumerate}  
Balancing reliability and usefulness thus becomes a dual objective: attain the desired coverage \emph{and} minimize the expected size of $\Gamma(\bm{x})$—often called the method’s \emph{efficiency}.  This coverage–efficiency trade-off provides a unifying lens under which conformal prediction extends seamlessly to both classification and regression contexts.


\paragraph{PAC-Style Bounds.}
Within the Probably Approximately Correct (PAC) framework, one can analyze selective classification by bounding the probability that the error on the covered set exceeds a given threshold. Define the \emph{selective risk} and \emph{coverage} of a previously introduced selective classifier $(f,g)$ by
\[
    R(f,g)
    \;:=\;
    \frac{\mathbb{E}\bigl[\mathds{1}\{g(X)=1\}\,\ell(f(X),Y)\bigr]}
         {\mathbb{E}\bigl[\mathds{1}\{g(X)=1\}\bigr]}
    ,\qquad
    \text{cov}(f,g)
    \;:=\;
    \mathbb{E}\bigl[\mathds{1}\{g(X)=1\}\bigr].
\]
Under standard PAC assumptions (i.i.d.\ samples, bounded loss), there exist $\epsilon,\alpha>0$ such that, with probability at least $1-\delta$ over $n$ training examples,
\begin{equation}
    R(f,g)\;\le\;\epsilon
    \quad\text{and}\quad
    \text{cov}(f,g)\;\ge\;1-\alpha,
    \label{eq:pac_selective}
\end{equation}
provided $n$ is large enough~\citep{cortes2016learning}.  
This result shows that by permitting an $\alpha$-fraction of rejections, one can drive the conditional error on accepted instances below $\epsilon$, given sufficient data to learn the gating function.



\subsection{Common Selective Prediction Techniques}

\paragraph{Softmax Response (\sr).} A traditional baseline method for selective prediction is the \emph{Softmax Response} (\sr) method~\citep{hendrycks2016baseline, geifman2017selective}. This method uses the confidence of a prediction model \(f\) as the selection score:
\begin{equation}
	g_{\sr}(\bm{x}, f) = \max_{c \in C} f(\bm{x})_c
\end{equation}
Here, \(f(\bm{x}) \in \mathbb{R}^{|C|}\) denotes the vector of predicted class probabilities (or logits) for input \(\bm{x}\), and \(f(\bm{x})_c\) refers to the score assigned to class \(c\). While this method is easy to implement and incurs no additional computational cost, \sr has been found to be overconfident on ambiguous, hard-to-classify, or unrecognizable inputs.

\paragraph{SelectiveNet (\sn).} 
A variety of selective classification methods have been proposed that leverage explicit architecture and loss function adaptations. For example, \emph{SelectiveNet} (\sn) \citep{geifman2019selectivenet} modifies the model architecture to jointly optimize $(f,g)$ while targeting the model at a desired coverage level~$c_\text{target}$. The augmented model consists of a representation function $r: \mathcal{X} \rightarrow \mathbb{R}^L$ mapping inputs to latent codes and three additional functions: (i) the \emph{prediction} function $f: \mathbb{R}^L \rightarrow \mathbb{R}^C$ for the classification task targeted at~$c_\text{target}$; (ii) the \emph{selection} function $g: \mathbb{R}^L \rightarrow [0,1]$ representing a continuous approximation of the accept/reject decision for $\bm{x}$; and (iii) an additional \emph{auxiliary} function $h: \mathbb{R}^L \rightarrow \mathbb{R}^C$ trained for the unconstrained classification tasks. This yields the following losses:
 \begin{align}
 	\mathcal{L} & = \alpha \mathcal{L}_{f,g} + (1-\alpha) \mathcal{L}_h \\
 	\mathcal{L}_{f,g} & = \frac{\frac{1}{M}\sum_{m=1}^{M} \ell( f \circ r(\bm{x}_m) , y_m)}{\text{cov}(f,g)} + \lambda \max(0, c - \text{cov}(f,g))^2 \\
 	\mathcal{L}_{h} & = \frac{1}{M}\sum_{m=1}^{M} \ell( h \circ r(\bm{x}_m) , y_m)
 \end{align}
 The selection score for a particular point $\bm{x}$ is then given by:
 \begin{equation}
 	g_{\sn}(\bm{x}, f) = \sigma(g \circ r(\bm{x})) = \frac{1}{1 + \exp(g \circ r(\bm{x}))}
 \end{equation}

 \paragraph{Self-Adaptive Training (\sat).}
 Alternatively, prior works like Deep Gamblers~\citep{liu2019deep} and \emph{Self-Adaptive Training}~\citep{huang2020self} have also considered explicitly modeling the abstention class $\bot$ and adapting the optimization process to provide a learning signal for this class. For instance, \emph{Self-Adaptive Training} (\sat) incorporates information obtained during the training process into the optimization itself by computing and monitoring an exponential moving average of training point predictions over the training process. Samples with high prediction uncertainty are then used for training the abstention class. To ensure that the exponential moving average captures the true prediction uncertainty, an initial burn-in phase is added to the training procedure. This delay allows the model to first optimize the non-augmented, \ie original $C$-class prediction task and optimize for selective classification during the remainder of the training process. The updated loss is defined as:
 \begin{equation}
 	\mathcal{L} = -\frac{1}{M}\sum_{m=1}^{M} \left ( t_{i,y_i}\log p_{i,y_i} + (1-t_{i,y_i})\log p_{i,C+1} \right )
 \end{equation}
The abstention decision is then determined by the degree of confidence in the rejection class:
\begin{equation}
	g_{\sat}(\bm{x}, f) = f(\bm{x})_{C+1}
\end{equation}

\paragraph{Deep Ensembles (\de).}
Ensemble methods combine the information content of $M$ models into a single final model. Since these models approximate the variance of the underlying prediction problem, they are often used for the purpose of uncertainty quantification and, by extension, \selp. The canonical instance of this approach for deep learning based models, \emph{Deep Ensembles}~(\de)~\citep{lakshminarayanan2017simple}, trains multiple models from scratch with varying initializations using a proper scoring rule. Optionally, adversarial training can be used to enhance robustness and improve uncertainty estimates. Intuitively, adversarial training smooths the predictive distribution by encouraging the model to assign similar probabilities to neighboring inputs in an $\epsilon$-ball around the training data---especially in directions of high loss---which in turn leads to better-calibrated and more stable uncertainty estimates. In the context of deep ensembles, adversarial training also promotes diversity among the ensemble members, thereby improving selective prediction performance. After averaging the predictions made by the models, the softmax response (\sr) mechanism is applied:
\begin{equation}
    g_{\de}(\bm{x}, f) = \max_{c \in C} \frac{1}{M} \sum_{m=1}^{M} f_{\bm{\theta}_{m,T}}(\bm{x})_c.
\end{equation}

\paragraph{Monte-Carlo Dropout (\mcdo).}
To overcome the computational cost of estimating multiple models from scratch, \emph{Monte-Carlo Dropout} (\mcdo) \citep{gal2016dropout} allows for bootstrapping model uncertainty from a single dropout‐equipped network at test time. While dropout is predominantly used during training to regularize deep neural nets, it can also be applied at inference to yield random sub‐networks of the full model. Concretely, let \(f_{\theta}\) be our base model parameters and \(o\) the dropout probability. At test time, we draw \(Q\) independent dropout masks—each unit is kept with \(1 - o\) probability—producing \(Q\) thinned networks $\bigl\{\,f_{\theta,o}^{(q)}\bigr\}_{q=1}^{Q}$. For a given test input \(\bm{x}\), we compute each network’s softmax output \(f_{\theta,o}^{(q)}(\bm{x})\in\Delta^{|C|}\), average these probability vectors, and then take the maximum entry as our selection score:
\begin{equation}
  g_{\mcdo}(\bm{x},f)
  \;=\;
  \max_{c\in C}
  \frac{1}{Q}
  \sum_{q=1}^{Q}
    f_{\theta,o}^{(q)}(\bm{x})_{c}\,.
\end{equation}
This procedure efficiently approximates ensemble‐style uncertainty without retraining multiple models.  


\subsection{Challenges and Trade-offs}
In selective prediction, a model’s performance depends not only on its accuracy when it decides to predict but also on how frequently it abstains. This leads to a fundamental \emph{coverage-utility trade-off}: increasing coverage (i.e., predicting on more samples) can degrade average performance, whereas being too selective may ignore a large portion of the data. In addition, various practical factors such as calibration, cost specification, and the nature of the output space further complicate the deployment of selective classifiers in real-world applications.

\paragraph{Calibration vs.\ Thresholding.}
A common approach to selective prediction is to set a confidence threshold below which the model abstains. However, if the model’s confidence scores are poorly calibrated, even a well-chosen threshold may yield suboptimal coverage and performance. Calibration techniques (e.g., Platt scaling or isotonic regression) are often necessary to align predicted confidence with actual likelihoods of correctness. Striking the right balance between calibration and thresholding is crucial for optimizing both utility (accuracy on covered samples) and coverage.

\paragraph{Data Distribution Shifts.}
In practice, data encountered during deployment may differ substantially from the training distribution. Under such domain shifts, confidence estimates can become unreliable, leading the gating mechanism to either abstain too frequently or incorrectly cover samples outside the training distribution. Techniques that adapt or re-estimate confidence under shifting conditions—such as online calibration or domain adaptation—remain an active area of research, aiming to preserve the coverage-utility trade-off even in evolving environments.

\paragraph{Cost Specification.}
Selective prediction often relies on a clear notion of the relative costs of rejection and misclassification. However, specifying these costs can be non-trivial, as it depends on domain knowledge, user preferences, and application-specific constraints. For instance, in a medical setting, the cost of a missed diagnosis may far outweigh the inconvenience of additional tests. An inaccurate cost specification can skew the thresholding decisions, leading to suboptimal coverage and utility outcomes.

\paragraph{Complex Outputs.}
While selective classification is well-understood for simple categorical outputs, tasks such as semantic segmentation or other structured prediction problems pose additional difficulties. Rejecting an entire structured output may be too coarse, yet partial rejections introduce substantial complexity into model design and training protocols. Extending selective prediction methods to these richer output spaces requires careful definition of what abstaining means (e.g., rejecting certain regions of an image segmentation vs.\ rejecting the entire image) and how to maintain favorable coverage-utility characteristics.


\subsection{Relationship Between UQ and Selective Prediction}

Whereas uncertainty quantification (UQ) can exist independently—for instance, by reporting confidence intervals or predictive distributions—selective prediction leverages these uncertainty estimates to decide whether to abstain from making a prediction. When reliable confidence or uncertainty metrics are available, selective classification naturally follows as a strategy for reducing risk. In particular, tools from uncertainty quantification—such as predictive distributions, confidence intervals, or calibration measures—yield scores or metrics (e.g., variance, entropy, or calibration error) that help determine whether the model should provide a label or abstain. However, for these signals to be effective, they must be both \emph{accurate} and \emph{well-calibrated}: miscalibrated or otherwise inaccurate estimates of uncertainty can lead to poor coverage-accuracy trade-offs, ultimately reducing the model’s reliability.

In this sense, selective prediction is an \textbf{action} grounded in the \textbf{estimation} process provided by uncertainty quantification. The decision to abstain hinges critically on the fidelity of the underlying uncertainty estimates. Without well-calibrated uncertainty, even sophisticated abstention mechanisms can perform poorly, either abstaining unnecessarily or failing to abstain when the risk is high.

More broadly, uncertainty estimation acquires practical significance only when situated within a downstream decision-making framework. Uncertainty—such as a predictive variance or entropy value—is not inherently meaningful unless it guides a choice among competing actions. In many real-world scenarios, decision-making under uncertainty is the norm: a medical diagnosis may trigger treatment or further testing, a financial risk estimate may inform loan approval, and a self-driving car’s confidence in its environment perception may determine whether it continues autonomously or defers control. In such contexts, the value of uncertainty lies in how it informs risk-aware decision-making.

Selective prediction provides a concrete instantiation of this principle. It formalizes a decision problem in which the model must choose between predicting and abstaining for each input, effectively balancing coverage against accuracy. Here, uncertainty is not just a descriptive statistic—it becomes a functional component of the model’s behavior. By embedding uncertainty into the decision rule itself, selective prediction highlights how the utility of uncertainty is deeply intertwined with the structure of the decision problem. Uncertainty matters most when there is something at stake and when there are viable alternatives—such as deferring to a human or a more accurate model—that can mitigate the consequences of model error.
