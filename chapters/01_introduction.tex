




\chapter{Introduction}
\label{ch:introduction}

\noindent Machine learning (ML) has become a central pillar of modern technology, powering applications in a multitude of fields as diverse as healthcare~\citep{kotropoulos2009linear,sousa2009ordinal, challen2019artificial,guan2020bounded, mozannar2020consistent}, finance~\citep{9260038}, and transportation~\citep{ghodsi2021generating, tselentis2023usefulness}. These systems enable healthcare providers to diagnose diseases more accurately, financial institutions to tailor credit offers, and autonomous vehicles to navigate roads safely. Yet, as ML models increasingly transition from research environments to real-world settings, their reliability is under intensified scrutiny. As the stakes grow, so does the potential harm if models behave unexpectedly~\citep{amodei2016concrete, wiens2019no}: incorrect diagnoses can lead to life-threatening medical decisions, biased lending systems can exacerbate social inequities, and errors in autonomous driving technology can pose immediate safety hazards. These risks are further magnified by the complexity of large-scale data pipelines~\citep{pervaiz2019examining} and the often opaque nature of modern ML algorithms, which can make errors difficult to predict or detect. In addition, various industries are grappling with evolving regulatory and ethical standards~\citep{lo2020ethical, yaghini2024regulation} that demand greater accountability and transparency. Consequently, researchers and practitioners alike are seeking to build greater trust into these models from the ground up, spurring the development of rigorous frameworks and methods that address reliability, fairness, and interpretability.

Motivated by these risks, researchers and practitioners have begun to prioritize methods that enhance model trustworthiness and transparency. This has led to the emergence of the field of \textit{trustworthy machine learning}~\citep{li2023trustworthy}. Trustworthy machine learning aims to ensure that models not only excel at predictive performance but also uphold principles of reliability, fairness, privacy, and accountability. It encompasses subfields dedicated to these goals, such as \textit{robust machine learning}~\citep{szegedy2013intriguing, papernot2017practical, rahimian2022frameworks}, which focuses on creating models resilient to variations in data distributions and adversarial inputs; \textit{fairness and bias mitigation}~\citep{hardt2016equality, mehrabi2021survey}, which aims to detect and reduce inequities that arise when data or algorithms systematically disadvantage certain groups; \textit{explainable or interpretable machine learning}~\citep{chen2019looks, rudin2019stop}, which seeks methods and frameworks to make complex model reasoning understandable to humans; and \textit{privacy-preserving techniques}~\citep{dwork2014algorithmic, abadi2016deep}, including differential privacy and federated learning, that protect sensitive information while enabling effective model training. Taken together, these subfields represent a concerted effort within the ML community to manage risks, fulfill ethical obligations, and promote transparent, accountable systems that reliably support high-stakes decisions in real-world contexts.

A key challenge in making ML systems more reliable lies in \textit{quantifying uncertainty}~\citep{gal2016uncertainty, hullermeier2021aleatoric, gawlikowski2023survey}. Classical supervised learning methods often assume that models should output a single best prediction without any mechanism for indicating when this prediction might be wrong. In reality, however, uncertainty is an integral component of most prediction tasks. For instance, models trained on limited or noisy data can become overconfident in their predictions~\citep{guo2017calibration}, leading to harmful outcomes when deployed. Consider an autonomous vehicle navigating in dense fog or heavy rain — if the visual input is degraded, a well-calibrated uncertainty estimate can prompt the vehicle to slow down or safely stop rather than make an overconfident and potentially dangerous decision. In healthcare, a diagnostic model uncertain about whether a tumor is benign or malignant can flag the case for further review by a radiologist instead of issuing a definitive but incorrect diagnosis. In financial services, a model assessing loan applications can defer borderline cases to a human analyst if uncertainty is high, reducing the risk of unjust denials due to atypical applicant profiles. Consequently, techniques for quantifying and managing uncertainty are crucial to safe and responsible model deployment. These techniques can inform decisions about when a model is trustworthy enough to act on, when human oversight or additional information might be needed, and how to handle model predictions that are at high risk of being incorrect.

Once reliable uncertainty estimates are available, the key challenge becomes translating them into principled actions. Decision theory offers a rich toolkit—ranging from Bayesian risk minimization to distribution-robust optimization—for choosing actions that balance expected utility against worst-case losses when outcomes are uncertain~\citep{berger2013statistical,rahimian2022frameworks}. In the supervised-learning setting, one of the most effective ways to operationalize this balance is to endow a model with an explicit \emph{reject option}: when the predicted probability of error exceeds a user-defined risk tolerance, the model abstains and defers the case to a downstream process such as a human expert, a more powerful model, or additional data collection~\citep{bartlett2008classification,geifman2017selective}. This paradigm—variously termed selective prediction, confidence-based abstention, or classification with a reject/abstain option—has seen growing adoption in high-stakes domains because it couples predictive performance with an actionable safeguard. Central to our work is precisely this concept of \textit{selective prediction} (SP)~\citep{chow1957optimum,el2010foundations}—an approach that allows the system to self-diagnose when it is likely to err and to withhold a decision rather than commit to a potentially harmful action. By integrating uncertainty estimates with abstention mechanisms, SP provides a formal pathway for aligning model behavior with real-world risk constraints, thereby transforming raw uncertainty into concrete, auditable decision rules that enhance safety and trustworthiness.


% - What to do with an uncertainty estimate?
% - Decision making under uncertainty, how to operationalize decisions in light of uncertainty
% - Selective prediction / reject option / model abstention
% - Central to our approach is the concept of \emph{selective prediction} (SP)~\citep{chow1957optimum,el2010foundations}—which gives a model the option to abstain from making predictions it deems too risky.
% - 

% This thesis explores the theme of uncertainty-driven reliability in ML across several key dimensions. First, it highlights the importance of uncertainty quantification as a foundation for techniques that selectively abstain from making decisions on inputs that are likely to be misclassified. Second, it investigates how these methods interact with broader objectives such as differential privacy, where noise is introduced during training to protect data but may undermine conventional strategies for building reliable predictors. Third, it examines theoretical performance guarantees for classifiers that base their decisions on confidence estimates, thereby providing insights into how and when such methods can work optimally. Fourth, it draws attention to adversarial exploits that can co-opt uncertainty-based decision-making systems for nefarious purposes, emphasizing the need for mechanisms that ensure uncertainties are genuine rather than artificially induced. Finally, it tackles computational constraints, demonstrating how a smaller model and a larger model can be orchestrated to reduce resource usage without sacrificing reliability, by deferring complex tasks from the smaller model to the larger model when the smaller model is uncertain.

% In the following chapters, we delve into each of these aspects in detail. We begin by reviewing the core concepts of uncertainty quantification, calibration, and selective prediction methods. We then show how these concepts can be deployed and assessed to ensure practical reliability in real-world contexts. By synthesizing ideas from algorithm design, theoretical analysis, and empirical evaluation, this thesis aims to provide a holistic view of how to develop—and trust—machine learning models that can fail gracefully.

\vspace{15pt}
\section{Thesis Statement}
\label{sec:statement}

\looseness=-1
The overarching theme of this thesis is to embed \emph{uncertainty quantification} at the heart of reliable machine learning (ML) systems. Central to our approach is the concept of \emph{selective prediction} (SP)~\citep{chow1957optimum,el2010foundations}—which gives a model the option to abstain from making predictions it deems too risky. However, building trustworthy selective models is not merely a matter of refusing uncertain inputs. It also entails ensuring that the uncertainty estimates themselves are sound, that they remain robust under constraints such as differential privacy, and that they can be defended against adversarial exploitation. In addition, the practical costs of deploying large models at scale demand strategies for offloading routine tasks to smaller, more efficient models without compromising reliability. Below is a roadmap of how these ideas interconnect to form the core narrative of this thesis.

We begin by laying the groundwork for what it means to \emph{measure} and \emph{manage} uncertainty in ML. This includes surveying classical approaches to calibrating confidence, examining how confidence estimates can fail in practice, and discussing why selectively rejecting uncertain inputs is an effective tool for mitigating high-impact errors. From this foundation, we transition to an in-depth study of the role that a model’s \emph{training trajectory} can play in boosting selective prediction performance. Rather than modifying the model’s architecture or primary loss function, we show how signals derived from intermediate checkpoints enable the model to self-diagnose its own likely points of failure—ultimately leading to a powerful, domain-agnostic mechanism for abstention.

Because this proposed approach merely observes the training process without interfering with it, it inherits a crucial compatibility with privacy-preserving learning: under the post-processing property of differential privacy (DP)~\citep{dwork2006calibrating}, operations applied after training do not compromise privacy guarantees. This makes our checkpoint-based method an attractive solution for \emph{uncertainty quantification in privacy-sensitive contexts}. We therefore turn next to exploring how selective prediction methods behave under strict privacy constraints. Many real-world applications demand that model training preserve the confidentiality of sensitive data, and DP is a gold standard for ensuring this. In this part of the thesis, we show how conventional selective prediction techniques—especially those reliant on ensembling~\citep{lakshminarayanan2017simple}—can amplify privacy risks or degrade under DP noise. In contrast, our training-dynamics-based approach proves particularly robust. This investigation also surfaces a deeper challenge: classical performance metrics often fail to disentangle the effects of privacy noise from genuine model uncertainty. To address this, we propose a new evaluation framework that enables a more faithful measurement of selective prediction performance in the presence of DP noise, capturing trade-offs between privacy, utility, and abstention coverage more effectively.

% <<The limitations of conventional SP evaluation under varying full-coverage accuracy levels that surfaced in the privacy setting prompt us to take a more fundamental look at how selective prediction performance should be assessed. Our next step is thus a \emph{theoretical analysis of the coverage--accuracy trade-off}, a central quantity in selective prediction. Here, we aim to characterize the intrinsic performance limits of selective classifiers, showing that two key components—calibration quality and the distribution of confidence scores—determine how much a model can safely abstain while still maintaining high utility on accepted points. We derive empirically tight upper bounds on this trade-off, clarifying when current methods approach optimality and when their failures can be traced back to specific calibration issues. These insights provide a principled foundation for interpreting and improving empirical results across domains.>>


The empirical and conceptual shortcomings revealed in the privacy study motivate a deeper interrogation of what actually determines selective prediction quality. To that end, we study the \emph{coverage-uniform selective-classification gap}—the deviation between a model's realized accuracy–coverage curve and the perfect-ordering upper bound—and prove that this gap can always be decomposed into five interpretable components: Bayes noise, approximation error, ranking error, statistical noise, and a miscellaneous term that captures optimization mishaps and distribution shift. The resulting finite-sample “error budget” clarifies which levers matter most at different coverage levels: increasing capacity or distilling from a stronger teacher shrinks the approximation term, additional or repeated labels dampen Bayes noise, larger validation sets reduce statistical fluctuations, and shift-aware training tackles the miscellaneous residue. Crucially, the analysis shows that monotone post-hoc calibration—long a default remedy for miscalibration—cannot touch the ranking term because it preserves the ordering of scores; progress therefore requires methods that can \emph{re-rank} predictions. Experiments spanning toy data and large-scale vision confirm the theory: capacity limits and Bayes noise dominate the gap, temperature scaling fixes calibration but not ranking, and domain shift inflates the residual term unless explicitly addressed. By quantifying why and where models fall short of the oracle, this framework establishes principled targets for future algorithmic improvements.

This gap to ideal selective classification performance assumes that errors arise from legitimate limitations about the training and/or deployment setup—not from adversarial interference. Yet, in practice, the very mechanisms designed to improve reliability can themselves be exploited. This realization naturally leads us to investigate whether uncertainty, long viewed as a desirable and protective property in machine learning systems, might also be used \emph{maliciously}. Surprisingly, we find that confidence estimates can be easily deliberately manipulated to suppress uncertainty in targeted input regions or for specific user groups, thereby covertly denying service while maintaining strong overall performance. These subtle attacks are difficult to detect because the system continues to perform well on aggregate metrics, even as it systematically disadvantages certain cases. To counter this threat, we propose a verifiable inference mechanism that audits reported confidence and ensures that abstentions reflect genuine uncertainty. This work highlights a crucial lesson: secure and trustworthy ML requires not only sound estimation procedures, but also safeguards that ensure the integrity and authenticity of uncertainty itself.

Finally, trustworthiness in ML is not only about robustness but also about practicality. While large models often achieve superior performance, their real-world deployment is frequently constrained by latency, memory, and compute costs. This motivates the design of \emph{cascaded systems}, where smaller, more efficient models handle routine inputs, and only the most challenging cases are escalated to larger, more capable models. We explore how uncertainty estimation and calibration can be used to govern these deferral decisions, enabling systems to make intelligent trade-offs between efficiency and accuracy. These results illustrate that uncertainty is not merely a tool for abstention or caution, but also a valuable signal for structuring adaptive model pipelines that optimize reliability under resource constraints.

This leads us to the central thesis statement:
% \begin{quotation}
% \noindent
% \emph{}
% \end{quotation}
% \boxedblock{
% Ensuring genuine reliability in machine learning requires more than accurate predictions—it demands principled uncertainty estimation, robustness to privacy constraints, theoretical grounding for selective behavior, and safeguards against adversarial misuse. This thesis demonstrates that by improving how models abstain, defer, and measure confidence, we can bring machine learning closer to real-world deployment standards that prioritize safety, fairness, and trust.
% }

\boxedblock{Ensuring genuine reliability in machine learning requires more than accurate predictions—it demands principled uncertainty estimation, robustness to privacy constraints, theoretical grounding for selective behavior, and safeguards against adversarial misuse. This thesis demonstrates that training dynamics offer a powerful and underutilized signal for improving and guiding how models abstain, defer, and measure confidence. By leveraging such lightweight and architecture-agnostic cues, we move closer to machine learning systems that meet real-world deployment standards for safety, fairness, and trust.}



% \begin{quotation}
% \noindent
% \emph{``Achieving reliable machine learning entails more than improving predictive accuracy: it requires a principled approach to uncertainty quantification that can guide selective abstentions, withstand privacy constraints, offer verifiable performance bounds, resist adversarial tampering, and operate within practical resource limits. By weaving these components into a cohesive framework, we unlock the potential for ML systems to fail gracefully, adapt to shifting real-world conditions, and foster genuine trust in their outputs.''}
% \end{quotation}

% \begin{quotation}
% \noindent
% \emph{``Ensuring genuine reliability in machine learning demands more than robust predictive performance. It requires principled uncertainty estimation, rigorous attention to privacy constraints, theoretical insights into the coverage–accuracy trade-off, and vigilance against adversarial manipulation. By examining each of these dimensions, this thesis shows how carefully tailored refinements to selective prediction, calibration, and evaluation can push ML systems toward safer, more transparent, and ultimately trustworthy real-world deployment.''}
% \end{quotation}






\section{Thesis Outline}
\label{sec:outline}

We summarize the contributions this thesis makes below:
\begin{itemize}
    \item \textbf{In Chapter~\ref{ch:background}}, we present background concepts crucial for understanding reliable decision-making in machine learning. We cover established approaches to modeling uncertainty, describe how predictive confidence is estimated, and discuss the rationale for selective prediction as a way to ensure safety by rejecting high-risk inputs. We also examine how confidence calibration underlies many of these techniques, enabling models to produce uncertainty estimates aligned with true error rates.

    \item \textbf{In Chapter~\ref{ch:sptd}}~\citep{rabanser2022selective}, we investigate how one can implement selective prediction by examining a model’s evolution throughout training, focusing on signals that emerge when predictions change or stabilize. We show that, by studying these signals, it is possible to determine which data points the final model is likely to misclassify. This perspective allows for a mechanism that rejects uncertain inputs without modifying the model’s architecture or primary optimization objective. The approach is adaptable to various tasks and domains, since it only requires access to intermediate training outputs.

    \item \textbf{In Chapter~\ref{ch:sptd_dp}}~\citep{rabanser2023training}, we delve into the interplay between selective prediction and differential privacy. We analyze how injecting random noise during training—an essential requirement for protecting sensitive data—can adversely affect a model's confidence estimates, reducing the reliability of conventional selective predictors. We examine why certain methods become less effective or more vulnerable under privacy constraints and propose refinements that leverage multiple training snapshots or other strategies to maintain robust performance. We also suggest novel evaluation metrics that better reflect how both privacy and reliability goals are being balanced.

    % \item \textbf{In Chapter~\ref{ch:sc_bounds}}~\citep{rabanser2025what}, we provide a theoretical framework for understanding confidence-based selective classification. We dissect how the coverage–accuracy trade-off can be decomposed into two key factors: the distribution of confidence scores (i.e., how many data points the model is confident about) and the calibration quality (i.e., the alignment between confidence scores and actual correctness). We offer empirically tight bounds that indicate the best possible performance under these conditions. Furthermore, we discuss how deviations from the ideal scenario often stem from calibration deficiencies and present guidance for diagnosing and correcting such issues.

    \textbf{In Chapter \ref{ch:sc_bounds}}~\citep{rabanser2025what}, we take a principled look at what governs how close a selective predictor can come to its oracle ideal. We formalize the \emph{selective-classification gap}—the deviation between a model’s accuracy–coverage curve and the perfect-ordering frontier—and derive the first finite-sample decomposition that attributes this gap to five interpretable error sources: Bayes noise, approximation limits, ranking imperfections, statistical fluctuations, and miscellaneous factors such as optimization error or distribution shift. The resulting “error budget’’ clarifies which levers (e.g., capacity, ranking-aware calibration, additional labels, or shift-robust training) most effectively shrink the gap at different coverage levels. We further prove that monotone post-hoc calibration cannot close the ranking term, motivating algorithms that can re-order confidence scores, and we validate the theory on tasks ranging from synthetic two-moons to large-scale vision benchmarks.

    \item \textbf{In Chapter~\ref{ch:conf_guard}}~\citep{rabanser2025confidential}, we highlight a potential adversarial threat in selective prediction systems: malicious entities can artificially suppress model confidence in targeted regions or for specific groups while retaining high performance elsewhere. This tactic allows an attacker to deny services or introduce bias under the pretense of uncertainty. We describe how such manipulation can be carried out, and we develop a strategy to detect and deter artificially induced low confidence. Central to this solution is the need for mechanisms that can verify confidence values or otherwise ensure that the model's abstentions are genuinely driven by uncertainty rather than deliberate manipulation.

    % \item \textbf{In Chapter~\ref{ch:gatekeeper}}~\citep{rabanser2025gatekeeper}, we consider a resource-sensitive setting in which a smaller model and a larger model are deployed in tandem. The smaller model handles straightforward tasks, whereas the larger model is used for more challenging cases. We discuss how careful calibration of the smaller model's confidence can identify those instances it can tackle reliably, deferring other cases to the larger model. By optimizing this hand-off process, organizations can deploy systems that balance computational efficiency with strong predictive performance, ensuring that resources are spent only when they are most needed while preserving reliability.

    \item \textbf{In Chapter~\ref{ch:conclusion}}, we conclude by summarizing the contributions of the thesis and highlighting future directions. We reflect on open research questions in areas such as adaptive calibration, reliability and uncertainty in large models, new forms of adversarial threats, and privacy-preserving learning. We also consider the broader implications of adopting selective prediction at scale, including ethical considerations in sensitive or high-stakes applications.
\end{itemize}